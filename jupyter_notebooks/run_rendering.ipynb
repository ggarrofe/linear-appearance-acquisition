{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":122684,"status":"ok","timestamp":1661342098869,"user":{"displayName":"Guillem garrof√©","userId":"02696913160405475090"},"user_tz":-120},"id":"Kzgp6r3nmXY7","outputId":"b2e9925e-bc50-4e8e-c8e7-5c6aed907ecf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting configargparse\n","  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n","Installing collected packages: configargparse\n","Successfully installed configargparse-1.5.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wandb\n","  Downloading wandb-0.13.2-py2.py3-none-any.whl (1.8 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.8 MB 34.0 MB/s \n","\u001b[?25hCollecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 181 kB 81.4 MB/s \n","\u001b[?25hCollecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Collecting setproctitle\n","  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (6.0)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 56.4 MB/s \n","\u001b[?25hCollecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 63 kB 1.8 MB/s \n","\u001b[?25hCollecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 90.6 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 97.3 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 98.2 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 157 kB 63.3 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 156 kB 104.0 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=27141eab1bd6694e93e66669fc60fa0316af3adc28378036445cfa141584f9ec\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built pathtools\n","Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n","Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.2\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting open3d\n","  Downloading open3d-0.15.2-cp37-cp37m-manylinux_2_27_x86_64.whl (408.6 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 408.6 MB 28 kB/s \n","\u001b[?25hRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.7/dist-packages (from open3d) (57.4.0)\n","Requirement already satisfied: wheel>=0.36.0 in /usr/local/lib/python3.7/dist-packages (from open3d) (0.37.1)\n","Requirement already satisfied: ipywidgets>=7.6.0 in /usr/local/lib/python3.7/dist-packages (from open3d) (7.7.1)\n","Collecting jupyterlab==3.*,>=3.0.0\n","  Downloading jupyterlab-3.4.5-py3-none-any.whl (8.8 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 8.8 MB 65.7 MB/s \n","\u001b[?25hCollecting addict\n","  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Collecting pillow>=8.2.0\n","  Downloading Pillow-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3.1 MB 54.9 MB/s \n","\u001b[?25hRequirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.7/dist-packages (from open3d) (3.2.2)\n","Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from open3d) (1.3.5)\n","Collecting pyquaternion\n","  Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from open3d) (4.64.0)\n","Collecting jupyter-packaging~=0.10\n","  Downloading jupyter_packaging-0.12.2-py3-none-any.whl (15 kB)\n","Requirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.7/dist-packages (from open3d) (1.21.6)\n","Requirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.7/dist-packages (from open3d) (1.0.2)\n","Requirement already satisfied: pyyaml>=5.4.1 in /usr/local/lib/python3.7/dist-packages (from open3d) (6.0)\n","Collecting pygments>=2.7.4\n","  Downloading Pygments-2.13.0-py3-none-any.whl (1.1 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.1 MB 83.5 MB/s \n","\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (21.3)\n","Collecting jupyterlab-server~=2.10\n","  Downloading jupyterlab_server-2.15.1-py3-none-any.whl (54 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 54 kB 2.8 MB/s \n","\u001b[?25hRequirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (4.11.1)\n","Collecting jupyter-server~=1.16\n","  Downloading jupyter_server-1.18.1-py3-none-any.whl (344 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 344 kB 78.1 MB/s \n","\u001b[?25hRequirement already satisfied: notebook<7 in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (5.3.1)\n","Collecting tornado>=6.1.0\n","  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 423 kB 70.3 MB/s \n","\u001b[?25hRequirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (7.9.0)\n","Collecting nbclassic\n","  Downloading nbclassic-0.4.3-py3-none-any.whl (9.7 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 9.7 MB 67.2 MB/s \n","\u001b[?25hRequirement already satisfied: jinja2>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (2.11.3)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->open3d) (5.3.4)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->open3d) (3.0.2)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->open3d) (5.1.1)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->open3d) (3.6.1)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->open3d) (0.2.0)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.0->open3d) (6.1.12)\n","Collecting jedi>=0.10\n","  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.6 MB 67.1 MB/s \n","\u001b[?25hRequirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.*,>=3.0.0->open3d) (0.7.5)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.*,>=3.0.0->open3d) (0.2.0)\n","Requirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.*,>=3.0.0->open3d) (4.8.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.*,>=3.0.0->open3d) (4.4.2)\n","Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.*,>=3.0.0->open3d) (2.0.10)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->jupyterlab==3.*,>=3.0.0->open3d) (0.8.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.1->jupyterlab==3.*,>=3.0.0->open3d) (2.0.1)\n","Collecting deprecation\n","  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n","Collecting tomlkit\n","  Downloading tomlkit-0.11.4-py3-none-any.whl (35 kB)\n","Collecting setuptools>=40.8.0\n","  Downloading setuptools-65.3.0-py3-none-any.whl (1.2 MB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1.2 MB 75.4 MB/s \n","\u001b[?25hRequirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.13.3)\n","Requirement already satisfied: nbformat>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (5.4.0)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (1.8.0)\n","Collecting argon2-cffi\n","  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n","Requirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (23.2.1)\n","Collecting websocket-client\n","  Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 54 kB 3.0 MB/s \n","\u001b[?25hCollecting prometheus-client\n","  Downloading prometheus_client-0.14.1-py3-none-any.whl (59 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 59 kB 7.8 MB/s \n","\u001b[?25hCollecting nbconvert>=6.4.4\n","  Downloading nbconvert-7.0.0-py3-none-any.whl (271 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 271 kB 94.4 MB/s \n","\u001b[?25hCollecting anyio<4,>=3.1.0\n","  Downloading anyio-3.6.1-py3-none-any.whl (80 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 80 kB 10.6 MB/s \n","\u001b[?25hCollecting sniffio>=1.1\n","  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (4.1.1)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (2.10)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.6.0->open3d) (2.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (2.23.0)\n","Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (4.3.3)\n","Collecting jinja2>=2.1\n","  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 133 kB 81.8 MB/s \n","\u001b[?25hRequirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (2.10.3)\n","Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (4.12.0)\n","Collecting json5\n","  Downloading json5-0.9.10-py2.py3-none-any.whl (19 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (3.8.1)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (22.1.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (0.18.1)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (5.9.0)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3->open3d) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3->open3d) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3->open3d) (3.0.9)\n","Collecting nbclient>=0.5.0\n","  Downloading nbclient-0.6.7-py3-none-any.whl (71 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 71 kB 189 kB/s \n","\u001b[?25hRequirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.7.1)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (4.9.1)\n","Collecting mistune<3,>=2.0.3\n","  Downloading mistune-2.0.4-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (5.0.1)\n","Collecting jupyterlab-pygments\n","  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n","Collecting tinycss2\n","  Downloading tinycss2-1.1.1-py3-none-any.whl (21 kB)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (4.6.3)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (1.5.0)\n","Collecting traitlets>=4.3.1\n","  Downloading traitlets-5.3.0-py3-none-any.whl (106 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 106 kB 85.3 MB/s \n","\u001b[?25hCollecting nest-asyncio\n","  Downloading nest_asyncio-1.5.5-py3-none-any.whl (5.2 kB)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.2.0->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (2.16.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->open3d) (2022.2.1)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->jupyterlab==3.*,>=3.0.0->open3d) (1.15.0)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->jupyterlab==3.*,>=3.0.0->open3d) (0.2.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21->open3d) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21->open3d) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21->open3d) (1.7.3)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.3->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.7.0)\n","Collecting argon2-cffi-bindings\n","  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 86 kB 5.9 MB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (2.21)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.5.1)\n","Collecting notebook-shim>=0.1.0\n","  Downloading notebook_shim-0.1.0-py3-none-any.whl (13 kB)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (2022.6.15)\n","Installing collected packages: traitlets, tornado, nest-asyncio, tinycss2, sniffio, setuptools, pygments, nbclient, mistune, jupyterlab-pygments, jinja2, jedi, argon2-cffi-bindings, websocket-client, prometheus-client, nbconvert, argon2-cffi, anyio, jupyter-server, notebook-shim, json5, tomlkit, nbclassic, jupyterlab-server, deprecation, pyquaternion, pillow, jupyterlab, jupyter-packaging, addict, open3d\n","  Attempting uninstall: traitlets\n","    Found existing installation: traitlets 5.1.1\n","    Uninstalling traitlets-5.1.1:\n","      Successfully uninstalled traitlets-5.1.1\n","  Attempting uninstall: tornado\n","    Found existing installation: tornado 5.1.1\n","    Uninstalling tornado-5.1.1:\n","      Successfully uninstalled tornado-5.1.1\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 57.4.0\n","    Uninstalling setuptools-57.4.0:\n","      Successfully uninstalled setuptools-57.4.0\n","  Attempting uninstall: pygments\n","    Found existing installation: Pygments 2.6.1\n","    Uninstalling Pygments-2.6.1:\n","      Successfully uninstalled Pygments-2.6.1\n","  Attempting uninstall: mistune\n","    Found existing installation: mistune 0.8.4\n","    Uninstalling mistune-0.8.4:\n","      Successfully uninstalled mistune-0.8.4\n","  Attempting uninstall: jinja2\n","    Found existing installation: Jinja2 2.11.3\n","    Uninstalling Jinja2-2.11.3:\n","      Successfully uninstalled Jinja2-2.11.3\n","  Attempting uninstall: nbconvert\n","    Found existing installation: nbconvert 5.6.1\n","    Uninstalling nbconvert-5.6.1:\n","      Successfully uninstalled nbconvert-5.6.1\n","  Attempting uninstall: pillow\n","    Found existing installation: Pillow 7.1.2\n","    Uninstalling Pillow-7.1.2:\n","      Successfully uninstalled Pillow-7.1.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires tornado~=5.1.0, but you have tornado 6.2 which is incompatible.\n","flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\u001b[0m\n","Successfully installed addict-2.4.0 anyio-3.6.1 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 deprecation-2.1.0 jedi-0.18.1 jinja2-3.1.2 json5-0.9.10 jupyter-packaging-0.12.2 jupyter-server-1.18.1 jupyterlab-3.4.5 jupyterlab-pygments-0.2.2 jupyterlab-server-2.15.1 mistune-2.0.4 nbclassic-0.4.3 nbclient-0.6.7 nbconvert-7.0.0 nest-asyncio-1.5.5 notebook-shim-0.1.0 open3d-0.15.2 pillow-9.2.0 prometheus-client-0.14.1 pygments-2.13.0 pyquaternion-0.9.9 setuptools-65.3.0 sniffio-1.2.0 tinycss2-1.1.1 tomlkit-0.11.4 tornado-6.2 traitlets-5.3.0 websocket-client-1.3.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","pkg_resources","pygments","tornado"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting lpips\n","  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n","\u001b[K     |‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 53 kB 2.3 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (4.64.0)\n","Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.21.6)\n","Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (0.13.1+cu113)\n","Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.12.1+cu113)\n","Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.7.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->lpips) (4.1.1)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.2.1->lpips) (2.23.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.2.1->lpips) (9.2.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.2.1->lpips) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.2.1->lpips) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.2.1->lpips) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.2.1->lpips) (3.0.4)\n","Installing collected packages: lpips\n","Successfully installed lpips-0.1.4\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","! pip install configargparse\n","! pip install wandb\n","! pip install open3d\n","! pip install lpips"]},{"cell_type":"markdown","metadata":{"id":"PBdK0o3HTxzd"},"source":["## Run NeRF\n","WandB apikey: 209f6ac4375563c3d09904b96206a2f5f1d75c24\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8nR9SzdLnPyj","outputId":"819f3166-73a0-4f75-a046-ad5f9c9390df"},"outputs":[{"name":"stdout","output_type":"stream","text":["usage: main.py [-h] [-c CONFIG] [--mesh MESH] [--dataset_path DATASET_PATH]\n","               [--out_path OUT_PATH]\n","               [--dataset_type {synthetic,llff,tiny,meshroom,colmap}]\n","               [--factor FACTOR] [--batch_size BATCH_SIZE] [--shuffle SHUFFLE]\n","               [--N_samples N_SAMPLES] [--D_c D_C] [--W_c W_C] [--N_f N_F]\n","               [--lrate LRATE] [--lrate_decay LRATE_DECAY] [--N_iters N_ITERS]\n","               [--near NEAR] [--far FAR] [--raw_noise_std RAW_NOISE_STD]\n","               [--dataset_to_gpu] [--colab] [--colab_path COLAB_PATH] [--test]\n","main.py: error: argument --batch_size: invalid int value: '1024*8'\n"]}],"source":["! python3 drive/Othercomputers/MacBookPro/NeRF/main.py -c drive/Othercomputers/MacBookPro/NeRF/configs/lego_llff_colab.conf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKm4sJvS4xrs"},"outputs":[],"source":["! python3 drive/Othercomputers/MacBookPro/nerf-pytorch/run_nerf.py --config drive/Othercomputers/MacBookPro/nerf-pytorch/configs/orchids.txt"]},{"cell_type":"markdown","metadata":{"id":"uw3rtMXaUCnm"},"source":["## Run Ray tracing a mesh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7TOT06fhfvRf"},"outputs":[],"source":["! python3 drive/Othercomputers/MacBookPro/Ray-tracing_mesh/ray_tracing_mesh.py -c drive/Othercomputers/MacBookPro/Ray-tracing_mesh/configs/lego_llff.conf"]},{"cell_type":"markdown","metadata":{"id":"a5bqQ-56fIsL"},"source":["## Surface rendering"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1199154,"status":"ok","timestamp":1660377188913,"user":{"displayName":"Guillem garrof√©","userId":"02696913160405475090"},"user_tz":-120},"id":"HVdYBt1n2xEd","outputId":"af35a109-8bb9-4369-d7dc-b6a2a1ab29c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda\n","['test', 'val', 'train']\n","tcmalloc: large alloc 1843200000 bytes == 0x85ac000 @  0x7f558e0c3001 0x7f551d9781af 0x7f551d9cec23 0x7f551d9cfa87 0x7f551da71823 0x5936cc 0x548c51 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7f558dcbec87 0x5b621a\n","Loading val - 20 images...\n","Fileposes:  drive/Othercomputers/MacBookPro/drums/poses_bounds_val.npy\n","\tfrom val_1\n","\tLoaded image data (800, 800, 3, 20) [ 800.          800.         1238.06948234]\n","\tLoaded drive/Othercomputers/MacBookPro/drums val\n","\tChanged 240 items out of 1440 in array of shape (120, 3, 4)\n","\t\tFrom (0, 0, 0) to (19, 2, 3)\n","Loading train - 100 images...\n","Fileposes:  drive/Othercomputers/MacBookPro/drums/poses_bounds_train.npy\n","\tfrom train_1\n","tcmalloc: large alloc 1536000000 bytes == 0xd2008000 @  0x7f558e0c11e7 0x7f551d9780ce 0x7f551d9cecf5 0x7f551da7786d 0x7f551da7817f 0x7f551da782d0 0x4bc4ab 0x7f551d9b9944 0x59371f 0x515244 0x549576 0x593fce 0x548ae9 0x5127f1 0x549e0e 0x4bcb19 0x7f551d9b9944 0x59371f 0x515244 0x549576 0x593fce 0x548ae9 0x51566f 0x549e0e 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1\n","\tLoaded image data (800, 800, 3, 100) [ 800.          800.         1238.06948234]\n","\tLoaded drive/Othercomputers/MacBookPro/drums train\n","\tChanged 1200 items out of 1440 in array of shape (120, 3, 4)\n","\t\tFrom (20, 0, 0) to (119, 2, 3)\n","Generating rays: 100% 120/120 [00:20<00:00,  5.79pose/s]\n","tcmalloc: large alloc 3686400000 bytes == 0x7f53b4dd6000 @  0x7f558e0a3b6b 0x7f558e0c3379 0x7f551e660d57 0x7f551e64ebc3 0x7f55485596af 0x7f554855a020 0x7f554855a074 0x7f554855a1bf 0x7f55492e182b 0x7f5549344d42 0x7f5548a84e37 0x7f55492ea89e 0x7f55492ea923 0x7f5548dfb565 0x7f5548a97531 0x7f55494301a3 0x7f5548f0285c 0x7f554a421f19 0x7f554a422416 0x7f5548f54132 0x7f5570340563 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1\n","Creating datasets...\n","\tComputing view dirs for val...\n","\tCreating val dataset with rays (torch.Size([12800000, 9])) and images (torch.Size([12800000, 3])) - shuffle True\n","tcmalloc: large alloc 3072000000 bytes == 0x7f52fd426000 @  0x7f558e0a3b6b 0x7f558e0c3379 0x7f551e660d57 0x7f551e64ebc3 0x7f55485596af 0x7f554855a020 0x7f554855a074 0x7f5548a6bdef 0x7f55492ddb8b 0x7f55490291d3 0x7f55492b8b8f 0x7f5549066bf7 0x7f554859d72c 0x7f554859f1cf 0x7f55485a0b83 0x7f5548a33e96 0x7f5548a40ac9 0x7f55492de000 0x7f5548eef20c 0x7f554a3f6e4d 0x7f554a3f7503 0x7f5548f3f05b 0x7f55704c7c62 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","\tComputing view dirs for train...\n","tcmalloc: large alloc 4608000000 bytes == 0x7f51337ee000 @  0x7f558e0a3b6b 0x7f558e0c3379 0x7f551e660d57 0x7f551e64ebc3 0x7f55485596af 0x7f554855a020 0x7f554855a074 0x7f554855a1bf 0x7f55492e182b 0x7f5549344d42 0x7f5548a84e37 0x7f55492ea89e 0x7f55492ea923 0x7f5548dbf49c 0x7f554a339fe5 0x7f554a33a7b6 0x7f5548dfb565 0x7f5570362c92 0x593835 0x548c51 0x5127f1 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173\n","\tCreating train dataset with rays (torch.Size([64000000, 9])) and images (torch.Size([64000000, 3])) - shuffle True\n","Datasets created successfully\n","\n","Reading PLY file: drive/Othercomputers/MacBookPro/drums/meshed-poisson.ply[========================================] 100%\n","scene created\n","tcmalloc: large alloc 4608000000 bytes == 0x7f501ed60000 @  0x7f558e0a3b6b 0x7f558e0c3379 0x7f551e660d57 0x7f551e64ebc3 0x7f55485596af 0x7f554855a020 0x7f554855a074 0x7f554855a1bf 0x7f55492e182b 0x7f5549344d42 0x7f5548a84e37 0x7f55492ea89e 0x7f55492ea923 0x7f5548dbf49c 0x7f554a339fe5 0x7f554a33a7b6 0x7f5548dfb565 0x7f5570362c92 0x593835 0x548c51 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x511e2c 0x549576 0x604173 0x5f5506\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220813_200320-3jufn4qw\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcomfy-glade-684\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/guillemgarrofe/controllable-neural-rendering\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/guillemgarrofe/controllable-neural-rendering/runs/3jufn4qw\u001b[0m\n","  0% 0/200000 [00:00<?, ?iteration/s]WARNING - 2022-08-13 20:03:40,941 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:03:40,963 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:03:40,985 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:03:44,799 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:03:44,822 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:03:44,844 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","\n","  0% 0.00/528M [00:00<?, ?B/s]\u001b[A\n","  1% 4.37M/528M [00:00<00:11, 45.8MB/s]\u001b[A\n","  2% 9.90M/528M [00:00<00:10, 52.9MB/s]\u001b[A\n","  6% 29.5M/528M [00:00<00:04, 123MB/s] \u001b[A\n"," 10% 50.2M/528M [00:00<00:03, 160MB/s]\u001b[A\n"," 13% 69.8M/528M [00:00<00:02, 176MB/s]\u001b[A\n"," 17% 89.9M/528M [00:00<00:02, 188MB/s]\u001b[A\n"," 21% 110M/528M [00:00<00:02, 197MB/s] \u001b[A\n"," 24% 129M/528M [00:00<00:02, 197MB/s]\u001b[A\n"," 28% 148M/528M [00:00<00:02, 194MB/s]\u001b[A\n"," 32% 166M/528M [00:01<00:02, 182MB/s]\u001b[A\n"," 35% 185M/528M [00:01<00:01, 185MB/s]\u001b[A\n"," 38% 203M/528M [00:01<00:01, 183MB/s]\u001b[A\n"," 42% 220M/528M [00:01<00:01, 181MB/s]\u001b[A\n"," 45% 237M/528M [00:01<00:01, 174MB/s]\u001b[A\n"," 48% 254M/528M [00:01<00:01, 172MB/s]\u001b[A\n"," 51% 271M/528M [00:01<00:01, 174MB/s]\u001b[A\n"," 55% 289M/528M [00:01<00:01, 178MB/s]\u001b[A\n"," 59% 309M/528M [00:01<00:01, 187MB/s]\u001b[A\n"," 62% 327M/528M [00:01<00:01, 185MB/s]\u001b[A\n"," 65% 345M/528M [00:02<00:01, 176MB/s]\u001b[A\n"," 69% 362M/528M [00:02<00:01, 171MB/s]\u001b[A\n"," 72% 379M/528M [00:02<00:00, 173MB/s]\u001b[A\n"," 75% 395M/528M [00:02<00:00, 171MB/s]\u001b[A\n"," 78% 413M/528M [00:02<00:00, 176MB/s]\u001b[A\n"," 81% 430M/528M [00:02<00:00, 174MB/s]\u001b[A\n"," 85% 447M/528M [00:02<00:00, 172MB/s]\u001b[A\n"," 88% 464M/528M [00:02<00:00, 175MB/s]\u001b[A\n"," 91% 482M/528M [00:02<00:00, 177MB/s]\u001b[A\n"," 94% 499M/528M [00:03<00:00, 178MB/s]\u001b[A\n","100% 528M/528M [00:03<00:00, 174MB/s]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:03:56,576 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:03:56,598 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:03:56,622 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:04:00,770 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:04:00,800 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:04:00,823 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 20/200000 [01:32<145:37:09,  2.62s/iteration]WARNING - 2022-08-13 20:05:00,162 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:00,184 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:00,207 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:03,193 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:03,216 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:03,238 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:05:08,019 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:08,041 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:08,065 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:11,197 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:11,218 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:11,238 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 40/200000 [02:43<154:37:09,  2.78s/iteration]WARNING - 2022-08-13 20:06:10,769 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:10,791 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:10,812 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:14,634 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:14,655 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:14,677 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:06:19,358 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:19,381 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:19,402 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:22,536 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:22,556 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:22,578 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 60/200000 [03:54<145:56:28,  2.63s/iteration]WARNING - 2022-08-13 20:07:22,313 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:22,335 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:22,356 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:25,285 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:25,306 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:25,327 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:07:29,797 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:29,821 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:29,846 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:32,876 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:32,898 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:32,919 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 80/200000 [05:05<159:44:40,  2.88s/iteration]WARNING - 2022-08-13 20:08:32,558 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:32,582 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:32,606 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:36,609 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:36,632 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:36,656 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:08:41,610 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:41,634 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:41,658 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:45,044 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:45,067 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:45,091 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 100/200000 [06:17<144:45:30,  2.61s/iteration]WARNING - 2022-08-13 20:09:45,244 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:45,266 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:45,288 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:48,262 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:48,284 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:09:52,870 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:52,892 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:52,915 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:56,090 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:56,112 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:56,133 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 120/200000 [07:28<157:02:34,  2.83s/iteration]WARNING - 2022-08-13 20:10:56,066 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:10:56,089 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:10:56,110 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:10:59,924 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:10:59,945 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:10:59,970 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:11:04,668 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:11:04,691 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:11:04,716 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:11:07,911 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:11:07,933 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:11:07,955 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 140/200000 [08:39<146:50:29,  2.64s/iteration]WARNING - 2022-08-13 20:12:07,873 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:07,897 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:07,919 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:10,897 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:10,918 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:12:15,504 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:15,532 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:15,557 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:18,716 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:18,738 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:18,758 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 160/200000 [09:51<157:18:36,  2.83s/iteration]WARNING - 2022-08-13 20:13:18,555 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:18,577 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:18,598 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:22,448 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:22,469 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:22,493 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:13:27,123 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:27,147 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:27,171 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:30,387 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:30,409 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:30,431 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 180/200000 [11:01<144:23:23,  2.60s/iteration]WARNING - 2022-08-13 20:14:29,741 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:29,764 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:29,786 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:32,783 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:32,804 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:32,825 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:14:37,634 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:37,655 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:37,679 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:40,912 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:40,941 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:40,977 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  1% 2000/200000 [1:35:02<145:00:23,  2.64s/iteration]WARNING - 2022-08-13 21:38:30,967 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:30,990 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:31,013 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:33,967 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:33,991 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:34,012 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 21:38:38,597 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:38,622 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:38,645 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:42,359 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:42,381 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:42,405 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  2% 4000/200000 [3:06:58<144:49:51,  2.66s/iteration]WARNING - 2022-08-13 23:10:26,246 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:26,268 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:26,293 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:29,297 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:29,318 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:29,341 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 23:10:34,021 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:34,043 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:34,066 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:37,005 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:37,028 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:37,052 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  3% 6000/200000 [4:39:04<149:33:03,  2.78s/iteration]WARNING - 2022-08-14 00:42:32,859 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:32,883 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:32,907 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:35,886 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:35,907 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:35,929 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 00:42:40,504 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:40,526 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:40,549 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:43,512 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:43,534 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:43,555 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  4% 8000/200000 [6:11:17<141:50:48,  2.66s/iteration]WARNING - 2022-08-14 02:14:45,532 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:45,556 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:45,580 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:48,531 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:48,552 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:48,574 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 02:14:53,043 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:53,064 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:53,088 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:55,989 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:56,010 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:56,032 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  5% 10000/200000 [7:43:39<145:41:42,  2.76s/iteration]WARNING - 2022-08-14 03:47:08,046 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:08,067 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:08,088 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:11,063 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:11,084 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:11,106 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 03:47:15,603 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:15,626 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:15,654 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:18,666 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:18,690 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:18,711 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  6% 12000/200000 [9:16:15<140:18:35,  2.69s/iteration]WARNING - 2022-08-14 05:19:43,958 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:43,982 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:44,007 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:47,013 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:47,034 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:47,055 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 05:19:51,533 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:51,557 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:51,581 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:54,513 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:54,536 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:54,558 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  7% 14000/200000 [10:49:01<140:40:25,  2.72s/iteration]WARNING - 2022-08-14 06:52:29,585 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:29,608 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:29,635 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:32,720 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:32,743 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:32,765 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 06:52:37,360 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:37,382 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:37,405 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:40,419 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:40,442 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:40,465 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  8% 16000/200000 [12:22:24<138:14:29,  2.70s/iteration]WARNING - 2022-08-14 08:25:52,701 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:25:52,723 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:25:52,746 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:25:55,716 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:25:55,738 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 08:26:00,247 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:26:00,271 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:26:00,296 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:26:03,223 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:26:03,244 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:26:03,265 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  9% 18000/200000 [13:55:15<136:37:43,  2.70s/iteration]WARNING - 2022-08-14 09:58:43,463 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:43,487 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:43,511 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:46,613 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:46,636 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:46,659 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 09:58:51,263 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:51,286 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:51,309 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:54,255 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:54,277 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:54,298 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n"," 10% 20000/200000 [15:28:53<142:46:10,  2.86s/iteration]WARNING - 2022-08-14 11:32:21,098 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:21,119 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:21,141 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:24,088 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:24,109 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:24,130 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 11:32:28,576 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:28,599 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:28,624 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:32,362 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:32,385 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:32,409 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n"," 11% 22000/200000 [17:01:58<131:37:26,  2.66s/iteration]WARNING - 2022-08-14 13:05:26,302 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:26,324 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:26,348 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:29,337 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:29,360 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:29,383 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 13:05:33,916 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:33,938 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:33,961 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:36,917 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:36,939 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:36,961 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n"," 11% 22556/200000 [17:27:51<139:29:16,  2.83s/iteration]"]}],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/surface_rendering.py -c drive/Othercomputers/MacBookPro/Rendering/configs/surfrend_colab.conf"]},{"cell_type":"markdown","source":["# Voxelised Radiance Linear Mapping"],"metadata":{"id":"ddON5aSLRGgO"}},{"cell_type":"code","source":["! python3 drive/Othercomputers/MacBookPro/Rendering/voxelised_radiance_mapping.py -c drive/Othercomputers/MacBookPro/Rendering/configs/voxelised_radiancemap.conf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"puN3_H14RKVi","executionInfo":{"status":"ok","timestamp":1661252603345,"user_tz":-120,"elapsed":105200,"user":{"displayName":"Guillem garrof√©","userId":"02696913160405475090"}},"outputId":"134f79d8-64f9-4281-868a-adb88dca7590"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda\n","dataset to:  cpu\n","['train', 'val', 'test']\n","tcmalloc: large alloc 1551360000 bytes == 0x98b8000 @  0x7fa765f27001 0x7fa6f57dc1af 0x7fa6f5832c23 0x7fa6f5833a87 0x7fa6f58d5823 0x5936cc 0x548c51 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7fa765b22c87 0x5b621a\n","Loading train - 1 images...\n","Fileposes:  drive/Othercomputers/MacBookPro/microphone/poses_bounds_train.npy\n","\tfrom train_1\n","\tLoaded image data (800, 800, 3, 1) [ 800.          800.         1010.71837593]\n","\tLoaded drive/Othercomputers/MacBookPro/microphone train\n","\tChanged 12 items out of 1212 in array of shape (101, 3, 4)\n","\t\tFrom (0, 0, 0) to (0, 2, 3)\n","Loading val - 100 images...\n","Fileposes:  drive/Othercomputers/MacBookPro/microphone/poses_bounds_val.npy\n","\tfrom val_1\n","tcmalloc: large alloc 1536000000 bytes == 0xc1cba000 @  0x7fa765f251e7 0x7fa6f57dc0ce 0x7fa6f5832cf5 0x7fa6f58db86d 0x7fa6f58dc17f 0x7fa6f58dc2d0 0x4bc4ab 0x7fa6f581d944 0x59371f 0x515244 0x549576 0x593fce 0x548ae9 0x5127f1 0x549e0e 0x4bcb19 0x7fa6f581d944 0x59371f 0x515244 0x549576 0x593fce 0x548ae9 0x51566f 0x549e0e 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1\n","\tLoaded image data (800, 800, 3, 100) [ 800.          800.         1010.71837593]\n","\tLoaded drive/Othercomputers/MacBookPro/microphone val\n","\tChanged 1200 items out of 1212 in array of shape (101, 3, 4)\n","\t\tFrom (1, 0, 0) to (100, 2, 3)\n","Generating rays: 100% 101/101 [00:17<00:00,  5.88pose/s]\n","tcmalloc: large alloc 3102720000 bytes == 0x1202ba000 @  0x7fa765f07b6b 0x7fa765f27379 0x7fa6f64c4d57 0x7fa6f64b2bc3 0x7fa7203bd6af 0x7fa7203be020 0x7fa7203be074 0x7fa7203be1bf 0x7fa72114582b 0x7fa7211a8d42 0x7fa7208e8e37 0x7fa72114e89e 0x7fa72114e923 0x7fa720c5f565 0x7fa7208fb531 0x7fa7212941a3 0x7fa720d6685c 0x7fa722285f19 0x7fa722286416 0x7fa720db8132 0x7fa7481a4563 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1\n","Creating datasets...\n","\tComputing view dirs for train...\n","\tCreating train dataset with rays (torch.Size([640000, 9])) and images (torch.Size([640000, 3])) - shuffle True\n","tcmalloc: large alloc 3072000000 bytes == 0x68c28000 @  0x7fa765f07b6b 0x7fa765f27379 0x7fa6f64c4d57 0x7fa6f64b2bc3 0x7fa7203bd6af 0x7fa7203be020 0x7fa7203be074 0x7fa7208cfdef 0x7fa721141b8b 0x7fa720e8d1d3 0x7fa72111cb8f 0x7fa720ecabf7 0x7fa72040172c 0x7fa7204031cf 0x7fa720404b83 0x7fa720897e96 0x7fa7208a4ac9 0x7fa721142000 0x7fa720d5320c 0x7fa72225ae4d 0x7fa72225b503 0x7fa720da305b 0x7fa74832bc62 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","\tComputing view dirs for val...\n","tcmalloc: large alloc 4608000000 bytes == 0x7fa461ed4000 @  0x7fa765f07b6b 0x7fa765f27379 0x7fa6f64c4d57 0x7fa6f64b2bc3 0x7fa7203bd6af 0x7fa7203be020 0x7fa7203be074 0x7fa7203be1bf 0x7fa72114582b 0x7fa7211a8d42 0x7fa7208e8e37 0x7fa72114e89e 0x7fa72114e923 0x7fa720c2349c 0x7fa72219dfe5 0x7fa72219e7b6 0x7fa720c5f565 0x7fa7481c6c92 0x593835 0x548c51 0x5127f1 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173\n","\tCreating val dataset with rays (torch.Size([64000000, 9])) and images (torch.Size([64000000, 3])) - shuffle True\n","Datasets created successfully\n","\n","Reading PLY file: drive/Othercomputers/MacBookPro/microphone/meshed-poisson_filtered.ply[========================================] 100%\n","tcmalloc: large alloc 4608000000 bytes == 0x4af48000 @  0x7fa765f07b6b 0x7fa765f27379 0x7fa6f64c4d57 0x7fa6f64b2bc3 0x7fa7203bd6af 0x7fa7203be020 0x7fa7203be074 0x7fa7203be1bf 0x7fa72114582b 0x7fa7211a8d42 0x7fa7208e8e37 0x7fa72114e89e 0x7fa72114e923 0x7fa720c2349c 0x7fa72219dfe5 0x7fa72219e7b6 0x7fa720c5f565 0x7fa7481c6c92 0x593835 0x548c51 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x511e2c 0x549576 0x604173 0x5f5506\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","Prediction time: 0.9547123908996582 seconds\n","WARNING - 2022-08-23 11:02:34,952 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-23 11:02:34,991 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-23 11:02:35,016 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-23 11:02:35,039 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Figure(2000x700)\n","WARNING - 2022-08-23 11:02:37,165 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-23 11:02:37,197 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-23 11:02:37,215 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-23 11:02:37,235 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Figure(2000x700)\n"]}]},{"cell_type":"markdown","metadata":{"id":"3dcUI9ProFcD"},"source":["# Clusterised Radiance Linear Mapping"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":85245,"status":"ok","timestamp":1661250916239,"user":{"displayName":"Guillem garrof√©","userId":"02696913160405475090"},"user_tz":-120},"id":"TroGNWt02zPr","outputId":"551a4890-eb26-4c07-c701-cd73281765ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda\n","dataset to:  cpu\n","['train', 'val', 'test']\n","tcmalloc: large alloc 1551360000 bytes == 0x963a000 @  0x7f8c87c26001 0x7f8c174db1af 0x7f8c17531c23 0x7f8c17532a87 0x7f8c175d4823 0x5936cc 0x548c51 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7f8c87821c87 0x5b621a\n","Loading train - 1 images...\n","Fileposes:  drive/Othercomputers/MacBookPro/microphone/poses_bounds_train.npy\n","\tfrom train_1\n","\tLoaded image data (800, 800, 3, 1) [ 800.          800.         1010.71837593]\n","\tLoaded drive/Othercomputers/MacBookPro/microphone train\n","\tChanged 12 items out of 1212 in array of shape (101, 3, 4)\n","\t\tFrom (0, 0, 0) to (0, 2, 3)\n","Loading val - 100 images...\n","Fileposes:  drive/Othercomputers/MacBookPro/microphone/poses_bounds_val.npy\n","\tfrom val_1\n","tcmalloc: large alloc 1536000000 bytes == 0xc1a3c000 @  0x7f8c87c241e7 0x7f8c174db0ce 0x7f8c17531cf5 0x7f8c175da86d 0x7f8c175db17f 0x7f8c175db2d0 0x4bc4ab 0x7f8c1751c944 0x59371f 0x515244 0x549576 0x593fce 0x548ae9 0x5127f1 0x549e0e 0x4bcb19 0x7f8c1751c944 0x59371f 0x515244 0x549576 0x593fce 0x548ae9 0x51566f 0x549e0e 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1\n","\tLoaded image data (800, 800, 3, 100) [ 800.          800.         1010.71837593]\n","\tLoaded drive/Othercomputers/MacBookPro/microphone val\n","\tChanged 1200 items out of 1212 in array of shape (101, 3, 4)\n","\t\tFrom (1, 0, 0) to (100, 2, 3)\n","Generating rays: 100% 101/101 [00:16<00:00,  5.98pose/s]\n","tcmalloc: large alloc 3102720000 bytes == 0x12003c000 @  0x7f8c87c06b6b 0x7f8c87c26379 0x7f8c181c3d57 0x7f8c181b1bc3 0x7f8c420bc6af 0x7f8c420bd020 0x7f8c420bd074 0x7f8c420bd1bf 0x7f8c42e4482b 0x7f8c42ea7d42 0x7f8c425e7e37 0x7f8c42e4d89e 0x7f8c42e4d923 0x7f8c4295e565 0x7f8c425fa531 0x7f8c42f931a3 0x7f8c42a6585c 0x7f8c43f84f19 0x7f8c43f85416 0x7f8c42ab7132 0x7f8c69ea3563 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1\n","Creating datasets...\n","\tComputing view dirs for train...\n","\tCreating train dataset with rays (torch.Size([640000, 9])) and images (torch.Size([640000, 3])) - shuffle True\n","tcmalloc: large alloc 3072000000 bytes == 0x689aa000 @  0x7f8c87c06b6b 0x7f8c87c26379 0x7f8c181c3d57 0x7f8c181b1bc3 0x7f8c420bc6af 0x7f8c420bd020 0x7f8c420bd074 0x7f8c425cedef 0x7f8c42e40b8b 0x7f8c42b8c1d3 0x7f8c42e1bb8f 0x7f8c42bc9bf7 0x7f8c4210072c 0x7f8c421021cf 0x7f8c42103b83 0x7f8c42596e96 0x7f8c425a3ac9 0x7f8c42e41000 0x7f8c42a5220c 0x7f8c43f59e4d 0x7f8c43f5a503 0x7f8c42aa205b 0x7f8c6a02ac62 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","\tComputing view dirs for val...\n","tcmalloc: large alloc 4608000000 bytes == 0x7f8983bb0000 @  0x7f8c87c06b6b 0x7f8c87c26379 0x7f8c181c3d57 0x7f8c181b1bc3 0x7f8c420bc6af 0x7f8c420bd020 0x7f8c420bd074 0x7f8c420bd1bf 0x7f8c42e4482b 0x7f8c42ea7d42 0x7f8c425e7e37 0x7f8c42e4d89e 0x7f8c42e4d923 0x7f8c4292249c 0x7f8c43e9cfe5 0x7f8c43e9d7b6 0x7f8c4295e565 0x7f8c69ec5c92 0x593835 0x548c51 0x5127f1 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173\n","\tCreating val dataset with rays (torch.Size([64000000, 9])) and images (torch.Size([64000000, 3])) - shuffle True\n","Datasets created successfully\n","\n","Reading PLY file: drive/Othercomputers/MacBookPro/microphone/meshed-poisson_filtered.ply[========================================] 100%\n","tcmalloc: large alloc 4608000000 bytes == 0x4ac98000 @  0x7f8c87c06b6b 0x7f8c87c26379 0x7f8c181c3d57 0x7f8c181b1bc3 0x7f8c420bc6af 0x7f8c420bd020 0x7f8c420bd074 0x7f8c420bd1bf 0x7f8c42e4482b 0x7f8c42ea7d42 0x7f8c425e7e37 0x7f8c42e4d89e 0x7f8c42e4d923 0x7f8c4292249c 0x7f8c43e9cfe5 0x7f8c43e9d7b6 0x7f8c4295e565 0x7f8c69ec5c92 0x593835 0x548c51 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x511e2c 0x549576 0x604173 0x5f5506\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","Prediction time: 0.7728002071380615 seconds\n","WARNING - 2022-08-23 10:34:46,763 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-23 10:34:46,802 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-23 10:34:46,826 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-23 10:34:46,849 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Figure(2000x700)\n","WARNING - 2022-08-23 10:34:48,788 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-23 10:34:48,823 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-23 10:34:48,853 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-23 10:34:48,880 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Figure(2000x700)\n"]}],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/clusterised_radiance_mapping.py -c drive/Othercomputers/MacBookPro/Rendering/configs/clusterised_radiancemap.conf"]},{"cell_type":"markdown","metadata":{"id":"oozGNYuQjeyw"},"source":["# Reflectance Linear Mapping"]},{"cell_type":"code","execution_count":27,"metadata":{"id":"d_i9K5kWw7ZE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660911033647,"user_tz":-120,"elapsed":108970,"user":{"displayName":"Guillem garrof√©","userId":"02696913160405475090"}},"outputId":"d7db3eb4-3c11-4091-b6cd-67ea745e811c"},"outputs":[{"output_type":"stream","name":"stdout","text":["['/content/drive/Othercomputers/MacBookPro/Rendering', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '../', 'drive/Othercomputers/MacBookPro/', '../']\n","Using cuda\n","dataset to:  cpu\n","['val', 'train', 'test']\n","tcmalloc: large alloc 1551360000 bytes == 0x93ce000 @  0x7f2533ae5001 0x7f24c335a1af 0x7f24c33b0c23 0x7f24c33b1a87 0x7f24c3453823 0x5936cc 0x548c51 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7f25336e0c87 0x5b621a\n","Loading val - 100 poses, images (800x800) and lights...\n","\tChanged 1153 items out of 1212 in array of shape (101, 3, 4)\n","\t\tFrom (0, 0, 0) to (99, 2, 3)\n","Loading train - 1 poses, images (800x800) and lights...\n","\tChanged 12 items out of 1212 in array of shape (101, 3, 4)\n","\t\tFrom (100, 0, 0) to (100, 2, 3)\n","Generating rays: 100% 101/101 [00:18<00:00,  5.52pose/s]\n","tcmalloc: large alloc 3102720000 bytes == 0x11ea48000 @  0x7f2533ac5b6b 0x7f2533ae5379 0x7f24c4042d57 0x7f24c4030bc3 0x7f24edf3b6af 0x7f24edf3c020 0x7f24edf3c074 0x7f24edf3c1bf 0x7f24eecc382b 0x7f24eed26d42 0x7f24ee466e37 0x7f24eeccc89e 0x7f24eeccc923 0x7f24ee7dd565 0x7f24ee479531 0x7f24eee121a3 0x7f24ee8e485c 0x7f24efe03f19 0x7f24efe04416 0x7f24ee936132 0x7f2515d22563 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1\n","Creating datasets...\n","tcmalloc: large alloc 3072000000 bytes == 0x65b4c000 @  0x7f2533ac5b6b 0x7f2533ae5379 0x7f24c4042d57 0x7f24c4030bc3 0x7f24edf3b6af 0x7f24edf3c020 0x7f24edf3c074 0x7f24ee44ddef 0x7f24eecbfb8b 0x7f24eea0b1d3 0x7f24eec9ab8f 0x7f24eea48bf7 0x7f24edf7f72c 0x7f24edf811cf 0x7f24edf82b83 0x7f24ee415e96 0x7f24ee422ac9 0x7f24eecc0000 0x7f24ee8d120c 0x7f24efdd8e4d 0x7f24efdd9503 0x7f24ee92105b 0x7f2515ea9c62 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","tcmalloc: large alloc 1536000000 bytes == 0x7f23dade0000 @  0x7f2533ac5b6b 0x7f2533ae5379 0x7f24c4042d57 0x7f24c4030bc3 0x7f24edf3b6af 0x7f24edf3c020 0x7f24edf3c074 0x7f24ee44ddef 0x7f24eecbfb8b 0x7f24eea0b1d3 0x7f24eec9ab8f 0x7f24eea48bf7 0x7f24edf7f72c 0x7f24edf811cf 0x7f24edf82b83 0x7f24ee415e96 0x7f24ee422ac9 0x7f24eecc0000 0x7f24ee8d120c 0x7f24efdd8e4d 0x7f24efdd9503 0x7f24ee92105b 0x7f2515ea9c62 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","\tComputing view dirs for val...\n","tcmalloc: large alloc 4608000000 bytes == 0x7f222f9f0000 @  0x7f2533ac5b6b 0x7f2533ae5379 0x7f24c4042d57 0x7f24c4030bc3 0x7f24edf3b6af 0x7f24edf3c020 0x7f24edf3c074 0x7f24edf3c1bf 0x7f24eecc382b 0x7f24eed26d42 0x7f24ee466e37 0x7f24eeccc89e 0x7f24eeccc923 0x7f24ee7a149c 0x7f24efd1bfe5 0x7f24efd1c7b6 0x7f24ee7dd565 0x7f2515d44c92 0x593835 0x548c51 0x5127f1 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173\n","\tCreating val dataset with rays (torch.Size([64000000, 9])) and images (torch.Size([64000000, 3])) - shuffle True\n","\tComputing view dirs for train...\n","\tCreating train dataset with rays (torch.Size([640000, 9])) and images (torch.Size([640000, 3])) - shuffle True\n","Datasets created successfully\n","\n","Reading PLY file: drive/Othercomputers/MacBookPro/microphone_light/meshed-poisson_aligned.ply[========================================] 100%\n","Generating ray lights: 100% 100/100 [00:17<00:00,  5.57pose/s]\n","tcmalloc: large alloc 3072000000 bytes == 0x7f217683a000 @  0x7f2533ac5b6b 0x7f2533ae5379 0x7f24c4042d57 0x7f24c4030bc3 0x7f24edf3b6af 0x7f24edf3c020 0x7f24edf3c074 0x7f24edf3c1bf 0x7f24eecc382b 0x7f24eed26d42 0x7f24ee466e37 0x7f24eeccc89e 0x7f24eeccc923 0x7f24ee7dd565 0x7f24ee479531 0x7f24eee121a3 0x7f24ee8e485c 0x7f24efe03f19 0x7f24efe04416 0x7f24ee936132 0x7f2515d22563 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173\n","Generating ray lights: 100% 1/1 [00:00<00:00,  5.42pose/s]\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","evaluating...\n","WARNING - 2022-08-19 12:10:06,219 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 12:10:06,248 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 12:10:06,279 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 12:10:06,421 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Prediction time: 0.10827207565307617 seconds\n","WARNING - 2022-08-19 12:10:09,144 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 12:10:09,167 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 12:10:09,191 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 12:10:09,216 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"]}],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/reflectance_mapping.py -c drive/Othercomputers/MacBookPro/Rendering/configs/reflectancemap_colab.conf"]},{"cell_type":"markdown","source":["# Reflectance Linear Mapping with Spherical Harmonics Encoding"],"metadata":{"id":"to4s8b-hd3jK"}},{"cell_type":"code","source":["! python3 drive/Othercomputers/MacBookPro/Rendering/reflectance_mapping_sphharm.py -c drive/Othercomputers/MacBookPro/Rendering/configs/reflectancemap_colab.conf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yEoGZrhBd9TD","executionInfo":{"status":"ok","timestamp":1661327358931,"user_tz":-120,"elapsed":237397,"user":{"displayName":"Guillem garrof√©","userId":"02696913160405475090"}},"outputId":"2915f82a-802e-4ba0-84e0-1291f3d6c2fc"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["['/content/drive/Othercomputers/MacBookPro/Rendering', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '../', 'drive/Othercomputers/MacBookPro/', '../']\n","Using cuda\n","dataset to:  cpu\n","['val', 'train', 'test']\n","tcmalloc: large alloc 1551360000 bytes == 0x8e0c000 @  0x7f0624a0b001 0x7f05b42801af 0x7f05b42d6c23 0x7f05b42d7a87 0x7f05b4379823 0x5936cc 0x548c51 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7f0624606c87 0x5b621a\n","Loading val - 1 poses, images (800x800) and lights...\n","\tChanged 11 items out of 1212 in array of shape (101, 3, 4)\n","\t\tFrom (0, 0, 0) to (0, 2, 3)\n","Loading train - 100 poses, images (800x800) and lights...\n","\tChanged 1143 items out of 1212 in array of shape (101, 3, 4)\n","\t\tFrom (1, 0, 0) to (100, 2, 3)\n","Generating rays: 100% 101/101 [00:18<00:00,  5.32pose/s]\n","tcmalloc: large alloc 3102720000 bytes == 0x11e486000 @  0x7f06249ebb6b 0x7f0624a0b379 0x7f05b4f68d57 0x7f05b4f56bc3 0x7f05dee616af 0x7f05dee62020 0x7f05dee62074 0x7f05dee621bf 0x7f05dfbe982b 0x7f05dfc4cd42 0x7f05df38ce37 0x7f05dfbf289e 0x7f05dfbf2923 0x7f05df703565 0x7f05df39f531 0x7f05dfd381a3 0x7f05df80a85c 0x7f05e0d29f19 0x7f05e0d2a416 0x7f05df85c132 0x7f0606c48563 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1\n","Creating datasets...\n","\tComputing view dirs for val...\n","\tCreating val dataset with rays (torch.Size([640000, 9])) and images (torch.Size([640000, 3])) - shuffle True\n","tcmalloc: large alloc 3072000000 bytes == 0x7f0470430000 @  0x7f06249ebb6b 0x7f0624a0b379 0x7f05b4f68d57 0x7f05b4f56bc3 0x7f05dee616af 0x7f05dee62020 0x7f05dee62074 0x7f05df373def 0x7f05dfbe5b8b 0x7f05df9311d3 0x7f05dfbc0b8f 0x7f05df96ebf7 0x7f05deea572c 0x7f05deea71cf 0x7f05deea8b83 0x7f05df33be96 0x7f05df348ac9 0x7f05dfbe6000 0x7f05df7f720c 0x7f05e0cfee4d 0x7f05e0cff503 0x7f05df84705b 0x7f0606dcfc62 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","tcmalloc: large alloc 1536000000 bytes == 0x6817c000 @  0x7f06249ebb6b 0x7f0624a0b379 0x7f05b4f68d57 0x7f05b4f56bc3 0x7f05dee616af 0x7f05dee62020 0x7f05dee62074 0x7f05df373def 0x7f05dfbe5b8b 0x7f05df9311d3 0x7f05dfbc0b8f 0x7f05df96ebf7 0x7f05deea572c 0x7f05deea71cf 0x7f05deea8b83 0x7f05df33be96 0x7f05df348ac9 0x7f05dfbe6000 0x7f05df7f720c 0x7f05e0cfee4d 0x7f05e0cff503 0x7f05df84705b 0x7f0606dcfc62 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","\tComputing view dirs for train...\n","tcmalloc: large alloc 4608000000 bytes == 0x7f03020d0000 @  0x7f06249ebb6b 0x7f0624a0b379 0x7f05b4f68d57 0x7f05b4f56bc3 0x7f05dee616af 0x7f05dee62020 0x7f05dee62074 0x7f05dee621bf 0x7f05dfbe982b 0x7f05dfc4cd42 0x7f05df38ce37 0x7f05dfbf289e 0x7f05dfbf2923 0x7f05df6c749c 0x7f05e0c41fe5 0x7f05e0c427b6 0x7f05df703565 0x7f0606c6ac92 0x593835 0x548c51 0x5127f1 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173\n","\tCreating train dataset with rays (torch.Size([64000000, 9])) and images (torch.Size([64000000, 3])) - shuffle True\n","Datasets created successfully\n","\n","Reading PLY file: drive/Othercomputers/MacBookPro/drums_light/blendswapdrums_rotated.ply[========================================] 100%\n","Generating ray lights: 100% 1/1 [00:00<00:00,  5.33pose/s]\n","Generating ray lights: 100% 100/100 [00:18<00:00,  5.34pose/s]\n","tcmalloc: large alloc 3072000000 bytes == 0x7f0248f1a000 @  0x7f06249ebb6b 0x7f0624a0b379 0x7f05b4f68d57 0x7f05b4f56bc3 0x7f05dee616af 0x7f05dee62020 0x7f05dee62074 0x7f05dee621bf 0x7f05dfbe982b 0x7f05dfc4cd42 0x7f05df38ce37 0x7f05dfbf289e 0x7f05dfbf2923 0x7f05df703565 0x7f05df39f531 0x7f05dfd381a3 0x7f05df80a85c 0x7f05e0d29f19 0x7f05e0d2a416 0x7f05df85c132 0x7f0606c48563 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173\n","Running kmeans on cuda: 7it [00:41,  5.90s/it, center_shift=1.325977, num_clusters=511, tol=2.000000]\n","Filtered 48043600 points to 100\n","Computing linear mappings: 100% 512/512 [01:02<00:00,  8.20linear mapping/s]\n","Training time: 104.6337878704071 seconds. Including 62.444796085357666 of K-means training.\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","100% 528M/528M [00:04<00:00, 115MB/s]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","evaluating...\n","Spec boundaries\n","x shape torch.Size([640000, 56])\n","pos (0, 36)\n","spec (36, 20)\n","boundaries in kwarfs True\n","Spec boundaries\n","x shape torch.Size([640000, 56])\n","pos (0, 36)\n","diff (0, 36)\n","boundaries in kwargs True\n","WARNING - 2022-08-24 07:49:04,727 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-24 07:49:04,765 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-24 07:49:04,785 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Prediction time: 0.17482423782348633 seconds\n","Spec boundaries\n","x shape torch.Size([640000, 56])\n","pos (0, 36)\n","spec (36, 20)\n","boundaries in kwarfs True\n","Spec boundaries\n","x shape torch.Size([640000, 56])\n","pos (0, 36)\n","diff (0, 36)\n","boundaries in kwargs True\n","WARNING - 2022-08-24 07:49:08,332 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-24 07:49:08,366 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-24 07:49:08,387 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"]}]},{"cell_type":"markdown","source":["# Enhanced Reflectance Network"],"metadata":{"id":"2e66Hcgye53Y"}},{"cell_type":"code","source":["! python3 drive/Othercomputers/MacBookPro/Rendering/enhanced_reflectance_network.py -c drive/Othercomputers/MacBookPro/Rendering/configs/enhanced_reflectance_network.conf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RJHQFi7He92n","executionInfo":{"status":"ok","timestamp":1660915726653,"user_tz":-120,"elapsed":96974,"user":{"displayName":"Guillem garrof√©","userId":"02696913160405475090"}},"outputId":"f6a8f2dd-0ce6-4f08-b166-e4dbd864fcae"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda\n","dataset to:  cpu\n","['val', 'train', 'test']\n","tcmalloc: large alloc 1689600000 bytes == 0x9c74000 @  0x7f938cf8a001 0x7f931c83f1af 0x7f931c895c23 0x7f931c896a87 0x7f931c938823 0x5936cc 0x548c51 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7f938cb85c87 0x5b621a\n","Loading val - 10 poses, images (800x800) and lights...\n","\tChanged 116 items out of 1320 in array of shape (110, 3, 4)\n","\t\tFrom (0, 0, 0) to (9, 2, 3)\n","Loading train - 100 poses, images (800x800) and lights...\n","\tChanged 1145 items out of 1320 in array of shape (110, 3, 4)\n","\t\tFrom (10, 0, 0) to (109, 2, 3)\n","Generating rays: 100% 110/110 [00:20<00:00,  5.41pose/s]\n","tcmalloc: large alloc 3379200000 bytes == 0x7f91c6478000 @  0x7f938cf6ab6b 0x7f938cf8a379 0x7f931d527d57 0x7f931d515bc3 0x7f93474206af 0x7f9347421020 0x7f9347421074 0x7f93474211bf 0x7f93481a882b 0x7f934820bd42 0x7f934794be37 0x7f93481b189e 0x7f93481b1923 0x7f9347cc2565 0x7f934795e531 0x7f93482f71a3 0x7f9347dc985c 0x7f93492e8f19 0x7f93492e9416 0x7f9347e1b132 0x7f936f207563 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1\n","Creating datasets...\n","\tComputing view dirs for val...\n","\tCreating val dataset with rays (torch.Size([6400000, 9])) and images (torch.Size([6400000, 3])) - shuffle True\n","tcmalloc: large alloc 3072000000 bytes == 0x7f910eac8000 @  0x7f938cf6ab6b 0x7f938cf8a379 0x7f931d527d57 0x7f931d515bc3 0x7f93474206af 0x7f9347421020 0x7f9347421074 0x7f9347932def 0x7f93481a4b8b 0x7f9347ef01d3 0x7f934817fb8f 0x7f9347f2dbf7 0x7f934746472c 0x7f93474661cf 0x7f9347467b83 0x7f93478fae96 0x7f9347907ac9 0x7f93481a5000 0x7f9347db620c 0x7f93492bde4d 0x7f93492be503 0x7f9347e0605b 0x7f936f38ec62 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","tcmalloc: large alloc 1536000000 bytes == 0xb4ad4000 @  0x7f938cf6ab6b 0x7f938cf8a379 0x7f931d527d57 0x7f931d515bc3 0x7f93474206af 0x7f9347421020 0x7f9347421074 0x7f9347932def 0x7f93481a4b8b 0x7f9347ef01d3 0x7f934817fb8f 0x7f9347f2dbf7 0x7f934746472c 0x7f93474661cf 0x7f9347467b83 0x7f93478fae96 0x7f9347907ac9 0x7f93481a5000 0x7f9347db620c 0x7f93492bde4d 0x7f93492be503 0x7f9347e0605b 0x7f936f38ec62 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","\tComputing view dirs for train...\n","tcmalloc: large alloc 4608000000 bytes == 0x7f8f81f20000 @  0x7f938cf6ab6b 0x7f938cf8a379 0x7f931d527d57 0x7f931d515bc3 0x7f93474206af 0x7f9347421020 0x7f9347421074 0x7f93474211bf 0x7f93481a882b 0x7f934820bd42 0x7f934794be37 0x7f93481b189e 0x7f93481b1923 0x7f9347c8649c 0x7f9349200fe5 0x7f93492017b6 0x7f9347cc2565 0x7f936f229c92 0x593835 0x548c51 0x5127f1 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173\n","\tCreating train dataset with rays (torch.Size([64000000, 9])) and images (torch.Size([64000000, 3])) - shuffle True\n","Datasets created successfully\n","\n","Reading PLY file: drive/Othercomputers/MacBookPro/microphone_light/meshed-poisson_aligned.ply[========================================] 100%\n","Generating ray lights: 100% 10/10 [00:01<00:00,  5.68pose/s]\n","Generating ray lights: 100% 100/100 [00:18<00:00,  5.40pose/s]\n","tcmalloc: large alloc 3072000000 bytes == 0x7f8ec8d6a000 @  0x7f938cf6ab6b 0x7f938cf8a379 0x7f931d527d57 0x7f931d515bc3 0x7f93474206af 0x7f9347421020 0x7f9347421074 0x7f93474211bf 0x7f93481a882b 0x7f934820bd42 0x7f934794be37 0x7f93481b189e 0x7f93481b1923 0x7f9347cc2565 0x7f934795e531 0x7f93482f71a3 0x7f9347dc985c 0x7f93492e8f19 0x7f93492e9416 0x7f9347e1b132 0x7f936f207563 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mggarrofe\u001b[0m (\u001b[33mguillemgarrofe\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220819_132833-24q5mkmi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mruby-planet-706\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/guillemgarrofe/controllable-neural-rendering\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/guillemgarrofe/controllable-neural-rendering/runs/24q5mkmi\u001b[0m\n","Running kmeans on cuda: 2it [00:00,  3.07it/s, center_shift=0.151703, num_clusters=31, tol=0.500000]\n","Computing linear mappings:   9% 3/32 [00:03<00:35,  1.21s/linear mapping]\n","Traceback (most recent call last):\n","  File \"drive/Othercomputers/MacBookPro/Rendering/enhanced_reflectance_network.py\", line 190, in <module>\n","    device=device)\n","  File \"drive/Othercomputers/MacBookPro/Rendering/enhanced_reflectance_network.py\", line 68, in compute_inv\n","    gc.collect()\n","KeyboardInterrupt\n","\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced \u001b[33mruby-planet-706\u001b[0m: \u001b[34m\u001b[4mhttps://wandb.ai/guillemgarrofe/controllable-neural-rendering/runs/24q5mkmi\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n","\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20220819_132833-24q5mkmi/logs\u001b[0m\n","^C\n"]}]},{"cell_type":"markdown","source":["# Ablation Studies"],"metadata":{"id":"rrKza92xEcpx"}},{"cell_type":"markdown","source":["## No normals"],"metadata":{"id":"59Bgfj89Eev-"}},{"cell_type":"code","source":["! python3 drive/Othercomputers/MacBookPro/Rendering/ablations/clusterised_radiance_nonormals.py -c drive/Othercomputers/MacBookPro/Rendering/configs/clusterised_radiancemap.conf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_B_6qV2eEi1A","executionInfo":{"status":"ok","timestamp":1661270141155,"user_tz":-120,"elapsed":72900,"user":{"displayName":"Guillem garrof√©","userId":"02696913160405475090"}},"outputId":"448ca51e-28ba-42fe-8bf4-45b55696346c"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["path ['/content/drive/Othercomputers/MacBookPro/Rendering/ablations', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '../', 'drive/Othercomputers/MacBookPro/', 'drive/Othercomputers/MacBookPro/Rendering/']\n","Using cuda\n","dataset to:  cpu\n","['train', 'val', 'test']\n","tcmalloc: large alloc 1551360000 bytes == 0x99ac000 @  0x7f5ce9834001 0x7f5c790e91af 0x7f5c7913fc23 0x7f5c79140a87 0x7f5c791e2823 0x5936cc 0x548c51 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7f5ce942fc87 0x5b621a\n","Loading train - 1 images...\n","Fileposes:  drive/Othercomputers/MacBookPro/lego/poses_bounds_train.npy\n","\tfrom train_1\n","\tLoaded image data (800, 800, 3, 1) [ 800.          800.         1094.61880067]\n","\tLoaded drive/Othercomputers/MacBookPro/lego train\n","\tChanged 12 items out of 1212 in array of shape (101, 3, 4)\n","\t\tFrom (0, 0, 0) to (0, 2, 3)\n","Loading val - 100 images...\n","Fileposes:  drive/Othercomputers/MacBookPro/lego/poses_bounds_val.npy\n","\tfrom val_1\n","tcmalloc: large alloc 1536000000 bytes == 0xc1ee6000 @  0x7f5ce98321e7 0x7f5c790e90ce 0x7f5c7913fcf5 0x7f5c791e886d 0x7f5c791e917f 0x7f5c791e92d0 0x4bc4ab 0x7f5c7912a944 0x59371f 0x515244 0x549576 0x593fce 0x548ae9 0x5127f1 0x549e0e 0x4bcb19 0x7f5c7912a944 0x59371f 0x515244 0x549576 0x593fce 0x548ae9 0x51566f 0x549e0e 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1\n","\tLoaded image data (800, 800, 3, 100) [ 800.          800.         1094.61880067]\n","\tLoaded drive/Othercomputers/MacBookPro/lego val\n","\tChanged 1200 items out of 1212 in array of shape (101, 3, 4)\n","\t\tFrom (1, 0, 0) to (100, 2, 3)\n","Generating rays: 100% 101/101 [00:16<00:00,  5.97pose/s]\n","tcmalloc: large alloc 3102720000 bytes == 0x11fecc000 @  0x7f5ce9814b6b 0x7f5ce9834379 0x7f5c79dd1d57 0x7f5c79dbfbc3 0x7f5ca3cca6af 0x7f5ca3ccb020 0x7f5ca3ccb074 0x7f5ca3ccb1bf 0x7f5ca4a5282b 0x7f5ca4ab5d42 0x7f5ca41f5e37 0x7f5ca4a5b89e 0x7f5ca4a5b923 0x7f5ca456c565 0x7f5ca4208531 0x7f5ca4ba11a3 0x7f5ca467385c 0x7f5ca5b92f19 0x7f5ca5b93416 0x7f5ca46c5132 0x7f5ccbab1563 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1\n","Creating datasets...\n","\tComputing view dirs for train...\n","\tCreating train dataset with rays (torch.Size([640000, 9])) and images (torch.Size([640000, 3])) - shuffle True\n","tcmalloc: large alloc 3072000000 bytes == 0x68d1c000 @  0x7f5ce9814b6b 0x7f5ce9834379 0x7f5c79dd1d57 0x7f5c79dbfbc3 0x7f5ca3cca6af 0x7f5ca3ccb020 0x7f5ca3ccb074 0x7f5ca41dcdef 0x7f5ca4a4eb8b 0x7f5ca479a1d3 0x7f5ca4a29b8f 0x7f5ca47d7bf7 0x7f5ca3d0e72c 0x7f5ca3d101cf 0x7f5ca3d11b83 0x7f5ca41a4e96 0x7f5ca41b1ac9 0x7f5ca4a4f000 0x7f5ca466020c 0x7f5ca5b67e4d 0x7f5ca5b68503 0x7f5ca46b005b 0x7f5ccbc38c62 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","\tComputing view dirs for val...\n","tcmalloc: large alloc 4608000000 bytes == 0x7f59e577c000 @  0x7f5ce9814b6b 0x7f5ce9834379 0x7f5c79dd1d57 0x7f5c79dbfbc3 0x7f5ca3cca6af 0x7f5ca3ccb020 0x7f5ca3ccb074 0x7f5ca3ccb1bf 0x7f5ca4a5282b 0x7f5ca4ab5d42 0x7f5ca41f5e37 0x7f5ca4a5b89e 0x7f5ca4a5b923 0x7f5ca453049c 0x7f5ca5aaafe5 0x7f5ca5aab7b6 0x7f5ca456c565 0x7f5ccbad3c92 0x593835 0x548c51 0x5127f1 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173\n","\tCreating val dataset with rays (torch.Size([64000000, 9])) and images (torch.Size([64000000, 3])) - shuffle True\n","Datasets created successfully\n","\n","Reading PLY file: drive/Othercomputers/MacBookPro/lego/meshed-poisson.ply[========================================] 100%\n","tcmalloc: large alloc 3072000000 bytes == 0x1ed10000 @  0x7f5ce9814b6b 0x7f5ce9834379 0x7f5c79dd1d57 0x7f5c79dbfbc3 0x7f5ca3cca6af 0x7f5ca3ccb020 0x7f5ca3ccb074 0x7f5ca3ccb1bf 0x7f5ca4a5282b 0x7f5ca4ab5d42 0x7f5ca41f5e37 0x7f5ca4a5b89e 0x7f5ca4a5b923 0x7f5ca453049c 0x7f5ca5aaafe5 0x7f5ca5aab7b6 0x7f5ca456c565 0x7f5ccbad3c92 0x593835 0x548c51 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x511e2c 0x549576 0x604173 0x5f5506\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","Prediction time: 0.3200492858886719 seconds\n","WARNING - 2022-08-23 15:55:24,212 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-23 15:55:24,245 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-23 15:55:24,263 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Figure(2000x700)\n","WARNING - 2022-08-23 15:55:26,254 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-23 15:55:26,272 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Figure(2000x700)\n"]}]},{"cell_type":"markdown","metadata":{"id":"NNAwcSy0Bg3a"},"source":["# Residual Reflectance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NP64cGoHtdOl"},"outputs":[],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/residual_reflectance.py -c drive/Othercomputers/MacBookPro/Rendering/configs/resref_colab.conf"]},{"cell_type":"markdown","metadata":{"id":"vH5hJcZwjUiV"},"source":["# Reflectance Mapping Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"06tpJrprjXhH"},"outputs":[],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/reflectance_mapping_network.py -c drive/Othercomputers/MacBookPro/Rendering/configs/reflectancemap_net_colab.conf"]},{"cell_type":"markdown","metadata":{"id":"Oyrqih7B6X03"},"source":["# Cluster selector network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWm3eBYmjHF5"},"outputs":[],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/reflectance_clusterized_network.py -c drive/Othercomputers/MacBookPro/Rendering/configs/reflectancemap_net_colab.conf"]},{"cell_type":"markdown","metadata":{"id":"FfQx7dI56o4I"},"source":["# Linear Mapping + Residual MLP"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"e0o_N4q36we6"},"outputs":[],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/mapping_residual_reflectance.py -c drive/Othercomputers/MacBookPro/Rendering/configs/reflectancemap_net_colab.conf"]},{"cell_type":"markdown","metadata":{"id":"a0KW_rtpkGCj"},"source":["# Autodecoder"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KIjtKkOakJJG","outputId":"d943cced-e636-4c78-8b72-ded7d475c4a9"},"outputs":[{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Using cuda\n","['val', 'train', 'test']\n","tcmalloc: large alloc 1551360000 bytes == 0x9abe000 @  0x7f9121e92001 0x7f90b17471af 0x7f90b179dc23 0x7f90b179ea87 0x7f90b1840823 0x5936cc 0x548c51 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7f9121a8dc87 0x5b621a\n","Loading val - 1 poses, images (800x800) and lights...\n","\tChanged 11 items out of 1212 in array of shape (101, 3, 4)\n","\t\tFrom (0, 0, 0) to (0, 2, 3)\n","Loading train - 100 poses, images (800x800) and lights...\n","\tChanged 1143 items out of 1212 in array of shape (101, 3, 4)\n","\t\tFrom (1, 0, 0) to (100, 2, 3)\n","Generating rays: 100% 101/101 [00:18<00:00,  5.39pose/s]\n","tcmalloc: large alloc 3102720000 bytes == 0x11f138000 @  0x7f9121e72b6b 0x7f9121e92379 0x7f90b242fd57 0x7f90b241dbc3 0x7f90dc3286af 0x7f90dc329020 0x7f90dc329074 0x7f90dc3291bf 0x7f90dd0b082b 0x7f90dd113d42 0x7f90dc853e37 0x7f90dd0b989e 0x7f90dd0b9923 0x7f90dcbca565 0x7f90dc866531 0x7f90dd1ff1a3 0x7f90dccd185c 0x7f90de1f0f19 0x7f90de1f1416 0x7f90dcd23132 0x7f910410f563 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1\n","Creating datasets...\n","\tComputing view dirs for val...\n","\tCreating val dataset with rays (torch.Size([640000, 9])) and images (torch.Size([640000, 3])) - shuffle True\n","tcmalloc: large alloc 3072000000 bytes == 0x7f8f6d796000 @  0x7f9121e72b6b 0x7f9121e92379 0x7f90b242fd57 0x7f90b241dbc3 0x7f90dc3286af 0x7f90dc329020 0x7f90dc329074 0x7f90dc83adef 0x7f90dd0acb8b 0x7f90dcdf81d3 0x7f90dd087b8f 0x7f90dce35bf7 0x7f90dc36c72c 0x7f90dc36e1cf 0x7f90dc36fb83 0x7f90dc802e96 0x7f90dc80fac9 0x7f90dd0ad000 0x7f90dccbe20c 0x7f90de1c5e4d 0x7f90de1c6503 0x7f90dcd0e05b 0x7f9104296c62 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","tcmalloc: large alloc 1536000000 bytes == 0x68e2e000 @  0x7f9121e72b6b 0x7f9121e92379 0x7f90b242fd57 0x7f90b241dbc3 0x7f90dc3286af 0x7f90dc329020 0x7f90dc329074 0x7f90dc83adef 0x7f90dd0acb8b 0x7f90dcdf81d3 0x7f90dd087b8f 0x7f90dce35bf7 0x7f90dc36c72c 0x7f90dc36e1cf 0x7f90dc36fb83 0x7f90dc802e96 0x7f90dc80fac9 0x7f90dd0ad000 0x7f90dccbe20c 0x7f90de1c5e4d 0x7f90de1c6503 0x7f90dcd0e05b 0x7f9104296c62 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","\tComputing view dirs for train...\n","tcmalloc: large alloc 4608000000 bytes == 0x7f8dff436000 @  0x7f9121e72b6b 0x7f9121e92379 0x7f90b242fd57 0x7f90b241dbc3 0x7f90dc3286af 0x7f90dc329020 0x7f90dc329074 0x7f90dc3291bf 0x7f90dd0b082b 0x7f90dd113d42 0x7f90dc853e37 0x7f90dd0b989e 0x7f90dd0b9923 0x7f90dcb8e49c 0x7f90de108fe5 0x7f90de1097b6 0x7f90dcbca565 0x7f9104131c92 0x593835 0x548c51 0x5127f1 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173\n","\tCreating train dataset with rays (torch.Size([64000000, 9])) and images (torch.Size([64000000, 3])) - shuffle True\n","Datasets created successfully\n","\n","Reading PLY file: drive/Othercomputers/MacBookPro/drums_light/blendswapdrums_rotated.ply[========================================] 100%\n","Generating ray lights: 100% 1/1 [00:00<00:00,  5.58pose/s]\n","Generating ray lights: 100% 100/100 [00:17<00:00,  5.59pose/s]\n","tcmalloc: large alloc 3072000000 bytes == 0x7f8d46280000 @  0x7f9121e72b6b 0x7f9121e92379 0x7f90b242fd57 0x7f90b241dbc3 0x7f90dc3286af 0x7f90dc329020 0x7f90dc329074 0x7f90dc3291bf 0x7f90dd0b082b 0x7f90dd113d42 0x7f90dc853e37 0x7f90dd0b989e 0x7f90dd0b9923 0x7f90dcbca565 0x7f90dc866531 0x7f90dd1ff1a3 0x7f90dccd185c 0x7f90de1f0f19 0x7f90de1f1416 0x7f90dcd23132 0x7f910410f563 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: "]}],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/reflectance_autodecoder.py -c drive/Othercomputers/MacBookPro/Rendering/configs/autodecoder_colab.conf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SrmjZo4EAyNP"},"outputs":[],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/reflectance_autoencoder.py -c drive/Othercomputers/MacBookPro/Rendering/configs/autodecoder_colab.conf"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":206,"status":"ok","timestamp":1658579073545,"user":{"displayName":"Guillem garrof√©","userId":"02696913160405475090"},"user_tz":-60},"id":"bEKtpUsLkEDj","outputId":"12905460-5c0d-4e2a-e60d-73e4c572ab53"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [7, 8, 9]])\n","tensor([2, 1, 0, 1, 0])\n","tensor([0, 1, 4])\n"]}],"source":["import torch\n","t1 = torch.tensor([[1, 2, 3], [4, 5, 6], [1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","\n","unique, inverse = torch.unique(t1, sorted=True, return_inverse=True, dim=0)\n","perm = torch.arange(inverse.size(0), dtype=inverse.dtype, device=inverse.device)\n","inverse, perm = inverse.flip([0]), perm.flip([0])\n","print(unique)\n","print(inverse)\n","perm = inverse.new_empty(unique.size(0)).scatter_(0, inverse, perm)\n","print(perm)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1658932255976,"user":{"displayName":"Guillem garrof√©","userId":"02696913160405475090"},"user_tz":-60},"id":"PN0hRKzC-9je","outputId":"49e9c7f2-b8cd-4d30-a966-5f6e40c6cf38"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-1.5066, -0.0317,  0.7404, -0.8176],\n","        [ 1.2103, -1.2459, -0.9263,  1.2217],\n","        [-0.0228, -1.3056,  0.7262,  0.2709],\n","        [-0.6637, -2.1896, -0.3487, -1.2596]])\n","tensor([[False, False,  True,  True],\n","        [False, False,  True,  True],\n","        [False, False,  True,  True],\n","        [False, False,  True,  True]])\n","tensor([2, 3, 2, 2])\n"]}],"source":["x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n","index = torch.tensor([0, 2])\n","x.index_fill_(0, index, -1)\n","\n","cluster_ids = []"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":401,"status":"ok","timestamp":1658937704446,"user":{"displayName":"Guillem garrof√©","userId":"02696913160405475090"},"user_tz":-60},"id":"bX9HgIyMo0Ye","outputId":"426065c6-d827-4cd3-8927-76cd7649537f"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[9.2120e-01, 6.5819e-01, 4.7914e-01, 1.4890e-01, 5.2660e-01, 6.3940e-01,\n","         9.8772e-01, 5.9303e-01, 7.0703e-01, 8.4739e-01],\n","        [8.5110e-01, 4.0239e-04, 2.8202e-01, 9.1530e-01, 1.8869e-01, 4.2283e-01,\n","         2.7864e-01, 1.2488e-01, 4.7079e-01, 2.5965e-01],\n","        [6.6420e-01, 7.4319e-01, 7.1035e-01, 9.8808e-01, 5.7236e-01, 3.9414e-01,\n","         9.3400e-05, 8.8030e-01, 5.3952e-01, 1.6256e-01],\n","        [9.9138e-01, 9.0262e-01, 4.6338e-01, 7.3467e-01, 6.1284e-01, 4.9806e-01,\n","         5.6239e-01, 3.8199e-01, 4.3856e-01, 4.6844e-01],\n","        [9.2781e-01, 5.1247e-01, 4.5681e-01, 5.4670e-01, 2.5513e-01, 3.3994e-01,\n","         3.1610e-01, 4.8427e-01, 7.8315e-01, 8.5255e-02]])\n","tensor([[2, 0, 1, 4, 3],\n","        [3, 1, 2, 5, 4],\n","        [4, 2, 3, 6, 5]])\n","tensor([[4.7914e-01, 1.4890e-01, 5.2660e-01],\n","        [8.5110e-01, 4.0239e-04, 2.8202e-01],\n","        [7.4319e-01, 7.1035e-01, 9.8808e-01],\n","        [6.1284e-01, 4.9806e-01, 5.6239e-01],\n","        [5.4670e-01, 2.5513e-01, 3.3994e-01]])\n","torch.Size([5, 3])\n"]}],"source":["X = torch.tensor([[1., 2., 3.], [4., 5., 6.], [1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","cluster_ids = torch.arange(0, end=5)\n","cluster_ids = cluster_ids[torch.randperm(cluster_ids.shape[0])]\n","\n","values = torch.rand((5, 10))\n","print(values)\n","col_indices = torch.stack([cluster_ids, cluster_ids+1, cluster_ids+2], dim=0)\n","print(col_indices)\n","row_indices = torch.arange(5)\n","print(values[row_indices, col_indices].T)\n","print(values[row_indices, col_indices].T.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234,"status":"ok","timestamp":1659060347075,"user":{"displayName":"Guillem garrof√©","userId":"02696913160405475090"},"user_tz":-60},"id":"w661fgkcleJB","outputId":"68409c9d-d9c6-4107-9e24-5360be758cdc"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([10, 15])\n","torch.Size([3, 15])\n"]},{"data":{"text/plain":["tensor([[ 0.5149, -0.2283,  0.1728,  0.2384, -0.1754, -0.0055,  0.0579,  0.5413,\n","         -0.4568, -0.0634,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n","        [-0.4487, -0.1163,  0.1923, -0.0186,  0.7851, -0.2504,  0.1239, -0.3273,\n","          0.5835, -0.1806,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n","        [ 0.1145,  0.5559, -0.1027, -0.0907, -0.4760,  0.4144,  0.0624, -0.0064,\n","          0.0639,  0.3303,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","cluster_ids = torch.arange(0,10)\n","\n","X = torch.rand((10, 3))\n","\n","def get_x2cluster(X, cluster_ids, num_clusters):\n","    indices = torch.stack([torch.arange(0,cluster_ids.shape[0]), cluster_ids])\n","    clusters = torch.sparse_coo_tensor(indices, torch.ones((cluster_ids.shape[0],)), (cluster_ids.shape[0], num_clusters)).to_dense()\n","    print(clusters.shape)\n","    x2cluster = torch.linalg.pinv(X) @ clusters  \n","    print(x2cluster.shape)\n","    return x2cluster\n","\n","get_x2cluster(X, cluster_ids, 15)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlZSbBZyp4fF"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"run_rendering.ipynb","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.13"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}