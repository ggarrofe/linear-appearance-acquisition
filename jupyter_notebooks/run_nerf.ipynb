{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"executionInfo":{"elapsed":109513,"status":"ok","timestamp":1660902199090,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-120},"id":"Kzgp6r3nmXY7","outputId":"ec32460b-a0ff-410b-cfcc-e3ee7e0a26bd"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting configargparse\n","  Downloading ConfigArgParse-1.5.3-py3-none-any.whl (20 kB)\n","Installing collected packages: configargparse\n","Successfully installed configargparse-1.5.3\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting wandb\n","  Downloading wandb-0.13.1-py2.py3-none-any.whl (1.8 MB)\n","\u001b[K     |████████████████████████████████| 1.8 MB 7.3 MB/s \n","\u001b[?25hCollecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.9.5-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 46.4 MB/s \n","\u001b[?25hCollecting setproctitle\n","  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n","Collecting pathtools\n","  Downloading pathtools-0.1.2.tar.gz (11 kB)\n","Collecting shortuuid>=0.5.0\n","  Downloading shortuuid-1.0.9-py3-none-any.whl (9.4 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from wandb) (57.4.0)\n","Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n","Collecting GitPython>=1.0.0\n","  Downloading GitPython-3.1.27-py3-none-any.whl (181 kB)\n","\u001b[K     |████████████████████████████████| 181 kB 61.2 MB/s \n","\u001b[?25hRequirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n","Requirement already satisfied: protobuf<4.0dev,>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.17.3)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n","Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n","Collecting docker-pycreds>=0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Requirement already satisfied: Click!=8.0.0,>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n","Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n","Collecting gitdb<5,>=4.0.1\n","  Downloading gitdb-4.0.9-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.8 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython>=1.0.0->wandb) (4.1.1)\n","Collecting smmap<6,>=3.0.1\n","  Downloading smmap-5.0.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n","Collecting sentry-sdk>=1.0.0\n","  Downloading sentry_sdk-1.9.4-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 55.8 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.3-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 56.8 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.2-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 54.4 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.1-py2.py3-none-any.whl (157 kB)\n","\u001b[K     |████████████████████████████████| 157 kB 54.4 MB/s \n","\u001b[?25h  Downloading sentry_sdk-1.9.0-py2.py3-none-any.whl (156 kB)\n","\u001b[K     |████████████████████████████████| 156 kB 58.7 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pathtools\n","  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8806 sha256=3f1db41cdd078424c0d761cd10a7a14540674b09f219fc99ce9c275e65ba86df\n","  Stored in directory: /root/.cache/pip/wheels/3e/31/09/fa59cef12cdcfecc627b3d24273699f390e71828921b2cbba2\n","Successfully built pathtools\n","Installing collected packages: smmap, gitdb, shortuuid, setproctitle, sentry-sdk, pathtools, GitPython, docker-pycreds, wandb\n","Successfully installed GitPython-3.1.27 docker-pycreds-0.4.0 gitdb-4.0.9 pathtools-0.1.2 sentry-sdk-1.9.0 setproctitle-1.3.2 shortuuid-1.0.9 smmap-5.0.0 wandb-0.13.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting open3d\n","  Downloading open3d-0.15.2-cp37-cp37m-manylinux_2_27_x86_64.whl (408.6 MB)\n","\u001b[K     |████████████████████████████████| 408.6 MB 29 kB/s \n","\u001b[?25hCollecting jupyter-packaging~=0.10\n","  Downloading jupyter_packaging-0.12.2-py3-none-any.whl (15 kB)\n","Requirement already satisfied: pandas>=1.0 in /usr/local/lib/python3.7/dist-packages (from open3d) (1.3.5)\n","Collecting pyyaml>=5.4.1\n","  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n","\u001b[K     |████████████████████████████████| 596 kB 53.5 MB/s \n","\u001b[?25hCollecting jupyterlab==3.*,>=3.0.0\n","  Downloading jupyterlab-3.4.5-py3-none-any.whl (8.8 MB)\n","\u001b[K     |████████████████████████████████| 8.8 MB 37.1 MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn>=0.21 in /usr/local/lib/python3.7/dist-packages (from open3d) (1.0.2)\n","Collecting pyquaternion\n","  Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n","Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.7/dist-packages (from open3d) (3.2.2)\n","Requirement already satisfied: numpy>1.15 in /usr/local/lib/python3.7/dist-packages (from open3d) (1.21.6)\n","Collecting pygments>=2.7.4\n","  Downloading Pygments-2.13.0-py3-none-any.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 46.3 MB/s \n","\u001b[?25hCollecting addict\n","  Downloading addict-2.4.0-py3-none-any.whl (3.8 kB)\n","Requirement already satisfied: wheel>=0.36.0 in /usr/local/lib/python3.7/dist-packages (from open3d) (0.37.1)\n","Collecting pillow>=8.2.0\n","  Downloading Pillow-9.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n","\u001b[K     |████████████████████████████████| 3.1 MB 55.9 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.7/dist-packages (from open3d) (57.4.0)\n","Requirement already satisfied: ipywidgets>=7.6.0 in /usr/local/lib/python3.7/dist-packages (from open3d) (7.7.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from open3d) (4.64.0)\n","Collecting nbclassic\n","  Downloading nbclassic-0.4.3-py3-none-any.whl (9.7 MB)\n","\u001b[K     |████████████████████████████████| 9.7 MB 42.1 MB/s \n","\u001b[?25hCollecting tornado>=6.1.0\n","  Downloading tornado-6.2-cp37-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (423 kB)\n","\u001b[K     |████████████████████████████████| 423 kB 53.7 MB/s \n","\u001b[?25hCollecting jupyter-server~=1.16\n","  Downloading jupyter_server-1.18.1-py3-none-any.whl (344 kB)\n","\u001b[K     |████████████████████████████████| 344 kB 66.2 MB/s \n","\u001b[?25hRequirement already satisfied: jinja2>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (2.11.3)\n","Requirement already satisfied: ipython in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (7.9.0)\n","Requirement already satisfied: notebook<7 in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (5.3.1)\n","Collecting jupyterlab-server~=2.10\n","  Downloading jupyterlab_server-2.15.0-py3-none-any.whl (54 kB)\n","\u001b[K     |████████████████████████████████| 54 kB 2.7 MB/s \n","\u001b[?25hRequirement already satisfied: jupyter-core in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (4.11.1)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from jupyterlab==3.*,>=3.0.0->open3d) (21.3)\n","Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->open3d) (5.1.1)\n","Requirement already satisfied: ipykernel>=4.5.1 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->open3d) (5.3.4)\n","Requirement already satisfied: jupyterlab-widgets>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->open3d) (1.1.1)\n","Requirement already satisfied: ipython-genutils~=0.2.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->open3d) (0.2.0)\n","Requirement already satisfied: widgetsnbextension~=3.6.0 in /usr/local/lib/python3.7/dist-packages (from ipywidgets>=7.6.0->open3d) (3.6.1)\n","Requirement already satisfied: jupyter-client in /usr/local/lib/python3.7/dist-packages (from ipykernel>=4.5.1->ipywidgets>=7.6.0->open3d) (6.1.12)\n","Collecting jedi>=0.10\n","  Downloading jedi-0.18.1-py2.py3-none-any.whl (1.6 MB)\n","\u001b[K     |████████████████████████████████| 1.6 MB 45.7 MB/s \n","\u001b[?25hRequirement already satisfied: pexpect in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.*,>=3.0.0->open3d) (4.8.0)\n","Requirement already satisfied: decorator in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.*,>=3.0.0->open3d) (4.4.2)\n","Requirement already satisfied: prompt-toolkit<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.*,>=3.0.0->open3d) (2.0.10)\n","Requirement already satisfied: backcall in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.*,>=3.0.0->open3d) (0.2.0)\n","Requirement already satisfied: pickleshare in /usr/local/lib/python3.7/dist-packages (from ipython->jupyterlab==3.*,>=3.0.0->open3d) (0.7.5)\n","Requirement already satisfied: parso<0.9.0,>=0.8.0 in /usr/local/lib/python3.7/dist-packages (from jedi>=0.10->ipython->jupyterlab==3.*,>=3.0.0->open3d) (0.8.3)\n","Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from jinja2>=2.1->jupyterlab==3.*,>=3.0.0->open3d) (2.0.1)\n","Collecting setuptools>=40.8.0\n","  Downloading setuptools-65.1.0-py3-none-any.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 51.5 MB/s \n","\u001b[?25hCollecting deprecation\n","  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n","Collecting tomlkit\n","  Downloading tomlkit-0.11.4-py3-none-any.whl (35 kB)\n","Requirement already satisfied: nbformat>=5.2.0 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (5.4.0)\n","Requirement already satisfied: Send2Trash in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (1.8.0)\n","Collecting argon2-cffi\n","  Downloading argon2_cffi-21.3.0-py3-none-any.whl (14 kB)\n","Collecting prometheus-client\n","  Downloading prometheus_client-0.14.1-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 6.7 MB/s \n","\u001b[?25hCollecting anyio<4,>=3.1.0\n","  Downloading anyio-3.6.1-py3-none-any.whl (80 kB)\n","\u001b[K     |████████████████████████████████| 80 kB 8.3 MB/s \n","\u001b[?25hCollecting nbconvert>=6.4.4\n","  Downloading nbconvert-6.5.3-py3-none-any.whl (563 kB)\n","\u001b[K     |████████████████████████████████| 563 kB 58.5 MB/s \n","\u001b[?25hRequirement already satisfied: pyzmq>=17 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (23.2.0)\n","Collecting websocket-client\n","  Downloading websocket_client-1.3.3-py3-none-any.whl (54 kB)\n","\u001b[K     |████████████████████████████████| 54 kB 3.1 MB/s \n","\u001b[?25hRequirement already satisfied: terminado>=0.8.3 in /usr/local/lib/python3.7/dist-packages (from jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.13.3)\n","Collecting sniffio>=1.1\n","  Downloading sniffio-1.2.0-py3-none-any.whl (10 kB)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (2.10)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from anyio<4,>=3.1.0->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (4.1.1)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from jupyter-client->ipykernel>=4.5.1->ipywidgets>=7.6.0->open3d) (2.8.2)\n","Requirement already satisfied: importlib-metadata>=3.6 in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (4.12.0)\n","Requirement already satisfied: babel in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (2.10.3)\n","Collecting jinja2>=2.1\n","  Downloading Jinja2-3.1.2-py3-none-any.whl (133 kB)\n","\u001b[K     |████████████████████████████████| 133 kB 62.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (2.23.0)\n","Requirement already satisfied: jsonschema>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (4.3.3)\n","Collecting json5\n","  Downloading json5-0.9.10-py2.py3-none-any.whl (19 kB)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=3.6->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (3.8.1)\n","Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (5.9.0)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (22.1.0)\n","Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from jsonschema>=3.0.1->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (0.18.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3->open3d) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3->open3d) (1.4.4)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=3->open3d) (3.0.9)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (4.9.1)\n","Requirement already satisfied: bleach in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (5.0.1)\n","Requirement already satisfied: mistune<2,>=0.8.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.8.4)\n","Requirement already satisfied: pandocfilters>=1.4.1 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (1.5.0)\n","Collecting tinycss2\n","  Downloading tinycss2-1.1.1-py3-none-any.whl (21 kB)\n","Requirement already satisfied: defusedxml in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.7.1)\n","Collecting nbclient>=0.5.0\n","  Downloading nbclient-0.6.6-py3-none-any.whl (71 kB)\n","\u001b[K     |████████████████████████████████| 71 kB 184 kB/s \n","\u001b[?25hCollecting jupyterlab-pygments\n","  Downloading jupyterlab_pygments-0.2.2-py2.py3-none-any.whl (21 kB)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (4.6.3)\n","Requirement already satisfied: entrypoints>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.4)\n","Collecting traitlets>=4.3.1\n","  Downloading traitlets-5.3.0-py3-none-any.whl (106 kB)\n","\u001b[K     |████████████████████████████████| 106 kB 68.8 MB/s \n","\u001b[?25hCollecting nest-asyncio\n","  Downloading nest_asyncio-1.5.5-py3-none-any.whl (5.2 kB)\n","Requirement already satisfied: fastjsonschema in /usr/local/lib/python3.7/dist-packages (from nbformat>=5.2.0->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (2.16.1)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.0->open3d) (2022.1)\n","Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->jupyterlab==3.*,>=3.0.0->open3d) (0.2.5)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from prompt-toolkit<2.1.0,>=2.0.0->ipython->jupyterlab==3.*,>=3.0.0->open3d) (1.15.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21->open3d) (1.1.0)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21->open3d) (3.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21->open3d) (1.7.3)\n","Requirement already satisfied: ptyprocess in /usr/local/lib/python3.7/dist-packages (from terminado>=0.8.3->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.7.0)\n","Collecting argon2-cffi-bindings\n","  Downloading argon2_cffi_bindings-21.2.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (86 kB)\n","\u001b[K     |████████████████████████████████| 86 kB 4.5 MB/s \n","\u001b[?25hRequirement already satisfied: cffi>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from argon2-cffi-bindings->argon2-cffi->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (1.15.1)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (2.21)\n","Requirement already satisfied: webencodings in /usr/local/lib/python3.7/dist-packages (from bleach->nbconvert>=6.4.4->jupyter-server~=1.16->jupyterlab==3.*,>=3.0.0->open3d) (0.5.1)\n","Collecting notebook-shim>=0.1.0\n","  Downloading notebook_shim-0.1.0-py3-none-any.whl (13 kB)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->jupyterlab-server~=2.10->jupyterlab==3.*,>=3.0.0->open3d) (1.24.3)\n","Installing collected packages: traitlets, tornado, nest-asyncio, tinycss2, sniffio, setuptools, pygments, nbclient, jupyterlab-pygments, jinja2, jedi, argon2-cffi-bindings, websocket-client, prometheus-client, nbconvert, argon2-cffi, anyio, jupyter-server, notebook-shim, json5, tomlkit, nbclassic, jupyterlab-server, deprecation, pyyaml, pyquaternion, pillow, jupyterlab, jupyter-packaging, addict, open3d\n","  Attempting uninstall: traitlets\n","    Found existing installation: traitlets 5.1.1\n","    Uninstalling traitlets-5.1.1:\n","      Successfully uninstalled traitlets-5.1.1\n","  Attempting uninstall: tornado\n","    Found existing installation: tornado 5.1.1\n","    Uninstalling tornado-5.1.1:\n","      Successfully uninstalled tornado-5.1.1\n","  Attempting uninstall: setuptools\n","    Found existing installation: setuptools 57.4.0\n","    Uninstalling setuptools-57.4.0:\n","      Successfully uninstalled setuptools-57.4.0\n","  Attempting uninstall: pygments\n","    Found existing installation: Pygments 2.6.1\n","    Uninstalling Pygments-2.6.1:\n","      Successfully uninstalled Pygments-2.6.1\n","  Attempting uninstall: jinja2\n","    Found existing installation: Jinja2 2.11.3\n","    Uninstalling Jinja2-2.11.3:\n","      Successfully uninstalled Jinja2-2.11.3\n","  Attempting uninstall: nbconvert\n","    Found existing installation: nbconvert 5.6.1\n","    Uninstalling nbconvert-5.6.1:\n","      Successfully uninstalled nbconvert-5.6.1\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: pillow\n","    Found existing installation: Pillow 7.1.2\n","    Uninstalling Pillow-7.1.2:\n","      Successfully uninstalled Pillow-7.1.2\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","google-colab 1.0.0 requires tornado~=5.1.0, but you have tornado 6.2 which is incompatible.\n","flask 1.1.4 requires Jinja2<3.0,>=2.10.1, but you have jinja2 3.1.2 which is incompatible.\u001b[0m\n","Successfully installed addict-2.4.0 anyio-3.6.1 argon2-cffi-21.3.0 argon2-cffi-bindings-21.2.0 deprecation-2.1.0 jedi-0.18.1 jinja2-3.1.2 json5-0.9.10 jupyter-packaging-0.12.2 jupyter-server-1.18.1 jupyterlab-3.4.5 jupyterlab-pygments-0.2.2 jupyterlab-server-2.15.0 nbclassic-0.4.3 nbclient-0.6.6 nbconvert-6.5.3 nest-asyncio-1.5.5 notebook-shim-0.1.0 open3d-0.15.2 pillow-9.2.0 prometheus-client-0.14.1 pygments-2.13.0 pyquaternion-0.9.9 pyyaml-6.0 setuptools-65.1.0 sniffio-1.2.0 tinycss2-1.1.1 tomlkit-0.11.4 tornado-6.2 traitlets-5.3.0 websocket-client-1.3.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","pkg_resources","pygments","tornado"]}}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting lpips\n","  Downloading lpips-0.1.4-py3-none-any.whl (53 kB)\n","\u001b[K     |████████████████████████████████| 53 kB 1.9 MB/s \n","\u001b[?25hRequirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.12.1+cu113)\n","Requirement already satisfied: tqdm>=4.28.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (4.64.0)\n","Requirement already satisfied: torchvision>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (0.13.1+cu113)\n","Requirement already satisfied: scipy>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.7.3)\n","Requirement already satisfied: numpy>=1.14.3 in /usr/local/lib/python3.7/dist-packages (from lpips) (1.21.6)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.0->lpips) (4.1.1)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.2.1->lpips) (9.2.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.2.1->lpips) (2.23.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.2.1->lpips) (2022.6.15)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.2.1->lpips) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.2.1->lpips) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision>=0.2.1->lpips) (1.24.3)\n","Installing collected packages: lpips\n","Successfully installed lpips-0.1.4\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","! pip install configargparse\n","! pip install wandb\n","! pip install open3d\n","! pip install lpips"]},{"cell_type":"markdown","metadata":{"id":"PBdK0o3HTxzd"},"source":["## Run NeRF\n","WandB apikey: 209f6ac4375563c3d09904b96206a2f5f1d75c24\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8nR9SzdLnPyj","outputId":"819f3166-73a0-4f75-a046-ad5f9c9390df"},"outputs":[{"name":"stdout","output_type":"stream","text":["usage: main.py [-h] [-c CONFIG] [--mesh MESH] [--dataset_path DATASET_PATH]\n","               [--out_path OUT_PATH]\n","               [--dataset_type {synthetic,llff,tiny,meshroom,colmap}]\n","               [--factor FACTOR] [--batch_size BATCH_SIZE] [--shuffle SHUFFLE]\n","               [--N_samples N_SAMPLES] [--D_c D_C] [--W_c W_C] [--N_f N_F]\n","               [--lrate LRATE] [--lrate_decay LRATE_DECAY] [--N_iters N_ITERS]\n","               [--near NEAR] [--far FAR] [--raw_noise_std RAW_NOISE_STD]\n","               [--dataset_to_gpu] [--colab] [--colab_path COLAB_PATH] [--test]\n","main.py: error: argument --batch_size: invalid int value: '1024*8'\n"]}],"source":["! python3 drive/Othercomputers/MacBookPro/NeRF/main.py -c drive/Othercomputers/MacBookPro/NeRF/configs/lego_llff_colab.conf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VKm4sJvS4xrs"},"outputs":[],"source":["! python3 drive/Othercomputers/MacBookPro/nerf-pytorch/run_nerf.py --config drive/Othercomputers/MacBookPro/nerf-pytorch/configs/orchids.txt"]},{"cell_type":"markdown","metadata":{"id":"uw3rtMXaUCnm"},"source":["## Run Ray tracing a mesh"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7TOT06fhfvRf"},"outputs":[],"source":["! python3 drive/Othercomputers/MacBookPro/Ray-tracing_mesh/ray_tracing_mesh.py -c drive/Othercomputers/MacBookPro/Ray-tracing_mesh/configs/lego_llff.conf"]},{"cell_type":"markdown","metadata":{"id":"a5bqQ-56fIsL"},"source":["## Surface rendering"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1199154,"status":"ok","timestamp":1660377188913,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-120},"id":"HVdYBt1n2xEd","outputId":"af35a109-8bb9-4369-d7dc-b6a2a1ab29c7"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda\n","['test', 'val', 'train']\n","tcmalloc: large alloc 1843200000 bytes == 0x85ac000 @  0x7f558e0c3001 0x7f551d9781af 0x7f551d9cec23 0x7f551d9cfa87 0x7f551da71823 0x5936cc 0x548c51 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7f558dcbec87 0x5b621a\n","Loading val - 20 images...\n","Fileposes:  drive/Othercomputers/MacBookPro/drums/poses_bounds_val.npy\n","\tfrom val_1\n","\tLoaded image data (800, 800, 3, 20) [ 800.          800.         1238.06948234]\n","\tLoaded drive/Othercomputers/MacBookPro/drums val\n","\tChanged 240 items out of 1440 in array of shape (120, 3, 4)\n","\t\tFrom (0, 0, 0) to (19, 2, 3)\n","Loading train - 100 images...\n","Fileposes:  drive/Othercomputers/MacBookPro/drums/poses_bounds_train.npy\n","\tfrom train_1\n","tcmalloc: large alloc 1536000000 bytes == 0xd2008000 @  0x7f558e0c11e7 0x7f551d9780ce 0x7f551d9cecf5 0x7f551da7786d 0x7f551da7817f 0x7f551da782d0 0x4bc4ab 0x7f551d9b9944 0x59371f 0x515244 0x549576 0x593fce 0x548ae9 0x5127f1 0x549e0e 0x4bcb19 0x7f551d9b9944 0x59371f 0x515244 0x549576 0x593fce 0x548ae9 0x51566f 0x549e0e 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1\n","\tLoaded image data (800, 800, 3, 100) [ 800.          800.         1238.06948234]\n","\tLoaded drive/Othercomputers/MacBookPro/drums train\n","\tChanged 1200 items out of 1440 in array of shape (120, 3, 4)\n","\t\tFrom (20, 0, 0) to (119, 2, 3)\n","Generating rays: 100% 120/120 [00:20<00:00,  5.79pose/s]\n","tcmalloc: large alloc 3686400000 bytes == 0x7f53b4dd6000 @  0x7f558e0a3b6b 0x7f558e0c3379 0x7f551e660d57 0x7f551e64ebc3 0x7f55485596af 0x7f554855a020 0x7f554855a074 0x7f554855a1bf 0x7f55492e182b 0x7f5549344d42 0x7f5548a84e37 0x7f55492ea89e 0x7f55492ea923 0x7f5548dfb565 0x7f5548a97531 0x7f55494301a3 0x7f5548f0285c 0x7f554a421f19 0x7f554a422416 0x7f5548f54132 0x7f5570340563 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1\n","Creating datasets...\n","\tComputing view dirs for val...\n","\tCreating val dataset with rays (torch.Size([12800000, 9])) and images (torch.Size([12800000, 3])) - shuffle True\n","tcmalloc: large alloc 3072000000 bytes == 0x7f52fd426000 @  0x7f558e0a3b6b 0x7f558e0c3379 0x7f551e660d57 0x7f551e64ebc3 0x7f55485596af 0x7f554855a020 0x7f554855a074 0x7f5548a6bdef 0x7f55492ddb8b 0x7f55490291d3 0x7f55492b8b8f 0x7f5549066bf7 0x7f554859d72c 0x7f554859f1cf 0x7f55485a0b83 0x7f5548a33e96 0x7f5548a40ac9 0x7f55492de000 0x7f5548eef20c 0x7f554a3f6e4d 0x7f554a3f7503 0x7f5548f3f05b 0x7f55704c7c62 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","\tComputing view dirs for train...\n","tcmalloc: large alloc 4608000000 bytes == 0x7f51337ee000 @  0x7f558e0a3b6b 0x7f558e0c3379 0x7f551e660d57 0x7f551e64ebc3 0x7f55485596af 0x7f554855a020 0x7f554855a074 0x7f554855a1bf 0x7f55492e182b 0x7f5549344d42 0x7f5548a84e37 0x7f55492ea89e 0x7f55492ea923 0x7f5548dbf49c 0x7f554a339fe5 0x7f554a33a7b6 0x7f5548dfb565 0x7f5570362c92 0x593835 0x548c51 0x5127f1 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173\n","\tCreating train dataset with rays (torch.Size([64000000, 9])) and images (torch.Size([64000000, 3])) - shuffle True\n","Datasets created successfully\n","\n","Reading PLY file: drive/Othercomputers/MacBookPro/drums/meshed-poisson.ply[========================================] 100%\n","scene created\n","tcmalloc: large alloc 4608000000 bytes == 0x7f501ed60000 @  0x7f558e0a3b6b 0x7f558e0c3379 0x7f551e660d57 0x7f551e64ebc3 0x7f55485596af 0x7f554855a020 0x7f554855a074 0x7f554855a1bf 0x7f55492e182b 0x7f5549344d42 0x7f5548a84e37 0x7f55492ea89e 0x7f55492ea923 0x7f5548dbf49c 0x7f554a339fe5 0x7f554a33a7b6 0x7f5548dfb565 0x7f5570362c92 0x593835 0x548c51 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x511e2c 0x549576 0x604173 0x5f5506\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220813_200320-3jufn4qw\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mcomfy-glade-684\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/guillemgarrofe/controllable-neural-rendering\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/guillemgarrofe/controllable-neural-rendering/runs/3jufn4qw\u001b[0m\n","  0% 0/200000 [00:00<?, ?iteration/s]WARNING - 2022-08-13 20:03:40,941 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:03:40,963 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:03:40,985 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:03:44,799 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:03:44,822 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:03:44,844 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n","\n","  0% 0.00/528M [00:00<?, ?B/s]\u001b[A\n","  1% 4.37M/528M [00:00<00:11, 45.8MB/s]\u001b[A\n","  2% 9.90M/528M [00:00<00:10, 52.9MB/s]\u001b[A\n","  6% 29.5M/528M [00:00<00:04, 123MB/s] \u001b[A\n"," 10% 50.2M/528M [00:00<00:03, 160MB/s]\u001b[A\n"," 13% 69.8M/528M [00:00<00:02, 176MB/s]\u001b[A\n"," 17% 89.9M/528M [00:00<00:02, 188MB/s]\u001b[A\n"," 21% 110M/528M [00:00<00:02, 197MB/s] \u001b[A\n"," 24% 129M/528M [00:00<00:02, 197MB/s]\u001b[A\n"," 28% 148M/528M [00:00<00:02, 194MB/s]\u001b[A\n"," 32% 166M/528M [00:01<00:02, 182MB/s]\u001b[A\n"," 35% 185M/528M [00:01<00:01, 185MB/s]\u001b[A\n"," 38% 203M/528M [00:01<00:01, 183MB/s]\u001b[A\n"," 42% 220M/528M [00:01<00:01, 181MB/s]\u001b[A\n"," 45% 237M/528M [00:01<00:01, 174MB/s]\u001b[A\n"," 48% 254M/528M [00:01<00:01, 172MB/s]\u001b[A\n"," 51% 271M/528M [00:01<00:01, 174MB/s]\u001b[A\n"," 55% 289M/528M [00:01<00:01, 178MB/s]\u001b[A\n"," 59% 309M/528M [00:01<00:01, 187MB/s]\u001b[A\n"," 62% 327M/528M [00:01<00:01, 185MB/s]\u001b[A\n"," 65% 345M/528M [00:02<00:01, 176MB/s]\u001b[A\n"," 69% 362M/528M [00:02<00:01, 171MB/s]\u001b[A\n"," 72% 379M/528M [00:02<00:00, 173MB/s]\u001b[A\n"," 75% 395M/528M [00:02<00:00, 171MB/s]\u001b[A\n"," 78% 413M/528M [00:02<00:00, 176MB/s]\u001b[A\n"," 81% 430M/528M [00:02<00:00, 174MB/s]\u001b[A\n"," 85% 447M/528M [00:02<00:00, 172MB/s]\u001b[A\n"," 88% 464M/528M [00:02<00:00, 175MB/s]\u001b[A\n"," 91% 482M/528M [00:02<00:00, 177MB/s]\u001b[A\n"," 94% 499M/528M [00:03<00:00, 178MB/s]\u001b[A\n","100% 528M/528M [00:03<00:00, 174MB/s]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:03:56,576 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:03:56,598 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:03:56,622 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:04:00,770 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:04:00,800 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:04:00,823 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 20/200000 [01:32<145:37:09,  2.62s/iteration]WARNING - 2022-08-13 20:05:00,162 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:00,184 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:00,207 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:03,193 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:03,216 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:03,238 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:05:08,019 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:08,041 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:08,065 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:11,197 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:11,218 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:05:11,238 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 40/200000 [02:43<154:37:09,  2.78s/iteration]WARNING - 2022-08-13 20:06:10,769 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:10,791 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:10,812 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:14,634 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:14,655 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:14,677 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:06:19,358 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:19,381 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:19,402 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:22,536 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:22,556 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:06:22,578 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 60/200000 [03:54<145:56:28,  2.63s/iteration]WARNING - 2022-08-13 20:07:22,313 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:22,335 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:22,356 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:25,285 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:25,306 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:25,327 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:07:29,797 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:29,821 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:29,846 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:32,876 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:32,898 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:07:32,919 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 80/200000 [05:05<159:44:40,  2.88s/iteration]WARNING - 2022-08-13 20:08:32,558 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:32,582 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:32,606 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:36,609 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:36,632 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:36,656 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:08:41,610 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:41,634 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:41,658 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:45,044 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:45,067 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:08:45,091 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 100/200000 [06:17<144:45:30,  2.61s/iteration]WARNING - 2022-08-13 20:09:45,244 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:45,266 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:45,288 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:48,262 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:48,284 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:09:52,870 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:52,892 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:52,915 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:56,090 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:56,112 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:09:56,133 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 120/200000 [07:28<157:02:34,  2.83s/iteration]WARNING - 2022-08-13 20:10:56,066 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:10:56,089 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:10:56,110 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:10:59,924 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:10:59,945 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:10:59,970 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:11:04,668 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:11:04,691 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:11:04,716 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:11:07,911 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:11:07,933 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:11:07,955 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 140/200000 [08:39<146:50:29,  2.64s/iteration]WARNING - 2022-08-13 20:12:07,873 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:07,897 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:07,919 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:10,897 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:10,918 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:12:15,504 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:15,532 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:15,557 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:18,716 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:18,738 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:12:18,758 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 160/200000 [09:51<157:18:36,  2.83s/iteration]WARNING - 2022-08-13 20:13:18,555 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:18,577 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:18,598 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:22,448 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:22,469 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:22,493 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:13:27,123 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:27,147 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:27,171 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:30,387 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:30,409 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:13:30,431 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  0% 180/200000 [11:01<144:23:23,  2.60s/iteration]WARNING - 2022-08-13 20:14:29,741 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:29,764 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:29,786 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:32,783 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:32,804 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:32,825 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 20:14:37,634 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:37,655 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:37,679 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:40,912 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:40,941 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 20:14:40,977 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  1% 2000/200000 [1:35:02<145:00:23,  2.64s/iteration]WARNING - 2022-08-13 21:38:30,967 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:30,990 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:31,013 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:33,967 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:33,991 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:34,012 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 21:38:38,597 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:38,622 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:38,645 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:42,359 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:42,381 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 21:38:42,405 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  2% 4000/200000 [3:06:58<144:49:51,  2.66s/iteration]WARNING - 2022-08-13 23:10:26,246 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:26,268 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:26,293 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:29,297 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:29,318 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:29,341 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-13 23:10:34,021 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:34,043 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:34,066 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:37,005 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:37,028 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-13 23:10:37,052 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  3% 6000/200000 [4:39:04<149:33:03,  2.78s/iteration]WARNING - 2022-08-14 00:42:32,859 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:32,883 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:32,907 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:35,886 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:35,907 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:35,929 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 00:42:40,504 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:40,526 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:40,549 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:43,512 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:43,534 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 00:42:43,555 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  4% 8000/200000 [6:11:17<141:50:48,  2.66s/iteration]WARNING - 2022-08-14 02:14:45,532 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:45,556 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:45,580 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:48,531 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:48,552 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:48,574 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 02:14:53,043 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:53,064 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:53,088 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:55,989 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:56,010 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 02:14:56,032 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  5% 10000/200000 [7:43:39<145:41:42,  2.76s/iteration]WARNING - 2022-08-14 03:47:08,046 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:08,067 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:08,088 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:11,063 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:11,084 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:11,106 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 03:47:15,603 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:15,626 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:15,654 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:18,666 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:18,690 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 03:47:18,711 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  6% 12000/200000 [9:16:15<140:18:35,  2.69s/iteration]WARNING - 2022-08-14 05:19:43,958 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:43,982 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:44,007 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:47,013 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:47,034 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:47,055 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 05:19:51,533 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:51,557 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:51,581 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:54,513 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:54,536 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 05:19:54,558 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  7% 14000/200000 [10:49:01<140:40:25,  2.72s/iteration]WARNING - 2022-08-14 06:52:29,585 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:29,608 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:29,635 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:32,720 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:32,743 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:32,765 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 06:52:37,360 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:37,382 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:37,405 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:40,419 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:40,442 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 06:52:40,465 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  8% 16000/200000 [12:22:24<138:14:29,  2.70s/iteration]WARNING - 2022-08-14 08:25:52,701 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:25:52,723 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:25:52,746 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:25:55,716 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:25:55,738 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 08:26:00,247 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:26:00,271 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:26:00,296 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:26:03,223 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:26:03,244 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 08:26:03,265 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","  9% 18000/200000 [13:55:15<136:37:43,  2.70s/iteration]WARNING - 2022-08-14 09:58:43,463 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:43,487 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:43,511 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:46,613 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:46,636 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:46,659 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 09:58:51,263 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:51,286 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:51,309 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:54,255 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:54,277 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 09:58:54,298 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n"," 10% 20000/200000 [15:28:53<142:46:10,  2.86s/iteration]WARNING - 2022-08-14 11:32:21,098 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:21,119 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:21,141 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:24,088 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:24,109 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:24,130 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 11:32:28,576 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:28,599 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:28,624 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:32,362 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:32,385 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 11:32:32,409 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n"," 11% 22000/200000 [17:01:58<131:37:26,  2.66s/iteration]WARNING - 2022-08-14 13:05:26,302 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:26,324 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:26,348 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:29,337 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:29,360 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:29,383 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","WARNING - 2022-08-14 13:05:33,916 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:33,938 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:33,961 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:36,917 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:36,939 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-14 13:05:36,961 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n"," 11% 22556/200000 [17:27:51<139:29:16,  2.83s/iteration]"]}],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/surface_rendering.py -c drive/Othercomputers/MacBookPro/Rendering/configs/surfrend_colab.conf"]},{"cell_type":"markdown","metadata":{"id":"3dcUI9ProFcD"},"source":["## Clusterised Radiance Linear Mapping"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":95616,"status":"ok","timestamp":1660863429742,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-120},"id":"TroGNWt02zPr","outputId":"66952e85-5df4-4bbd-f296-477d4bc4606b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda\n","dataset to:  cpu\n","['train', 'val', 'test']\n","tcmalloc: large alloc 1551360000 bytes == 0x9d96000 @  0x7f6fd326d001 0x7f6f62b221af 0x7f6f62b78c23 0x7f6f62b79a87 0x7f6f62c1b823 0x5936cc 0x548c51 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7f6fd2e68c87 0x5b621a\n","Loading train - 1 images...\n","Fileposes:  drive/Othercomputers/MacBookPro/microphone/poses_bounds_train.npy\n","\tfrom train_1\n","\tLoaded image data (800, 800, 3, 1) [ 800.          800.         1010.71837593]\n","\tLoaded drive/Othercomputers/MacBookPro/microphone train\n","\tChanged 12 items out of 1212 in array of shape (101, 3, 4)\n","\t\tFrom (0, 0, 0) to (0, 2, 3)\n","Loading val - 100 images...\n","Fileposes:  drive/Othercomputers/MacBookPro/microphone/poses_bounds_val.npy\n","\tfrom val_1\n","tcmalloc: large alloc 1536000000 bytes == 0xc2198000 @  0x7f6fd326b1e7 0x7f6f62b220ce 0x7f6f62b78cf5 0x7f6f62c2186d 0x7f6f62c2217f 0x7f6f62c222d0 0x4bc4ab 0x7f6f62b63944 0x59371f 0x515244 0x549576 0x593fce 0x548ae9 0x5127f1 0x549e0e 0x4bcb19 0x7f6f62b63944 0x59371f 0x515244 0x549576 0x593fce 0x548ae9 0x51566f 0x549e0e 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1\n","\tLoaded image data (800, 800, 3, 100) [ 800.          800.         1010.71837593]\n","\tLoaded drive/Othercomputers/MacBookPro/microphone val\n","\tChanged 1200 items out of 1212 in array of shape (101, 3, 4)\n","\t\tFrom (1, 0, 0) to (100, 2, 3)\n","Generating rays: 100% 101/101 [00:18<00:00,  5.53pose/s]\n","tcmalloc: large alloc 3102720000 bytes == 0x120798000 @  0x7f6fd324db6b 0x7f6fd326d379 0x7f6f6380ad57 0x7f6f637f8bc3 0x7f6f8d7036af 0x7f6f8d704020 0x7f6f8d704074 0x7f6f8d7041bf 0x7f6f8e48b82b 0x7f6f8e4eed42 0x7f6f8dc2ee37 0x7f6f8e49489e 0x7f6f8e494923 0x7f6f8dfa5565 0x7f6f8dc41531 0x7f6f8e5da1a3 0x7f6f8e0ac85c 0x7f6f8f5cbf19 0x7f6f8f5cc416 0x7f6f8e0fe132 0x7f6fb54ea563 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1\n","Creating datasets...\n","\tComputing view dirs for train...\n","\tCreating train dataset with rays (torch.Size([640000, 9])) and images (torch.Size([640000, 3])) - shuffle True\n","tcmalloc: large alloc 3072000000 bytes == 0x69106000 @  0x7f6fd324db6b 0x7f6fd326d379 0x7f6f6380ad57 0x7f6f637f8bc3 0x7f6f8d7036af 0x7f6f8d704020 0x7f6f8d704074 0x7f6f8dc15def 0x7f6f8e487b8b 0x7f6f8e1d31d3 0x7f6f8e462b8f 0x7f6f8e210bf7 0x7f6f8d74772c 0x7f6f8d7491cf 0x7f6f8d74ab83 0x7f6f8dbdde96 0x7f6f8dbeaac9 0x7f6f8e488000 0x7f6f8e09920c 0x7f6f8f5a0e4d 0x7f6f8f5a1503 0x7f6f8e0e905b 0x7f6fb5671c62 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","\tComputing view dirs for val...\n","tcmalloc: large alloc 4608000000 bytes == 0x7f6ccf1f0000 @  0x7f6fd324db6b 0x7f6fd326d379 0x7f6f6380ad57 0x7f6f637f8bc3 0x7f6f8d7036af 0x7f6f8d704020 0x7f6f8d704074 0x7f6f8d7041bf 0x7f6f8e48b82b 0x7f6f8e4eed42 0x7f6f8dc2ee37 0x7f6f8e49489e 0x7f6f8e494923 0x7f6f8df6949c 0x7f6f8f4e3fe5 0x7f6f8f4e47b6 0x7f6f8dfa5565 0x7f6fb550cc92 0x593835 0x548c51 0x5127f1 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173\n","\tCreating val dataset with rays (torch.Size([64000000, 9])) and images (torch.Size([64000000, 3])) - shuffle True\n","Datasets created successfully\n","\n","Reading PLY file: drive/Othercomputers/MacBookPro/microphone/meshed-poisson_filtered.ply[========================================] 100%\n","scene created\n","tcmalloc: large alloc 4608000000 bytes == 0x4b3f4000 @  0x7f6fd324db6b 0x7f6fd326d379 0x7f6f6380ad57 0x7f6f637f8bc3 0x7f6f8d7036af 0x7f6f8d704020 0x7f6f8d704074 0x7f6f8d7041bf 0x7f6f8e48b82b 0x7f6f8e4eed42 0x7f6f8dc2ee37 0x7f6f8e49489e 0x7f6f8e494923 0x7f6f8df6949c 0x7f6f8f4e3fe5 0x7f6f8f4e47b6 0x7f6f8dfa5565 0x7f6fb550cc92 0x593835 0x548c51 0x5127f1 0x549576 0x593fce 0x548ae9 0x5127f1 0x549576 0x593fce 0x511e2c 0x549576 0x604173 0x5f5506\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","Prediction time: 0.847928524017334 seconds\n","WARNING - 2022-08-18 22:56:38,240 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-18 22:56:38,285 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-18 22:56:38,313 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-18 22:56:38,343 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Figure(2000x700)\n","WARNING - 2022-08-18 22:56:41,271 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-18 22:56:41,312 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-18 22:56:41,339 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-18 22:56:41,365 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Figure(2000x700)\n"]}],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/clusterised_radiance_mapping.py -c drive/Othercomputers/MacBookPro/Rendering/configs/clusterised_radiancemap.conf"]},{"cell_type":"markdown","metadata":{"id":"oozGNYuQjeyw"},"source":["# Reflectance Linear Mapping"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"d_i9K5kWw7ZE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1660903405519,"user_tz":-120,"elapsed":151953,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"}},"outputId":"539bc2ac-e8fb-40c5-aabc-1960723effca"},"outputs":[{"output_type":"stream","name":"stdout","text":["['/content/drive/Othercomputers/MacBookPro/Rendering', '/env/python', '/usr/lib/python37.zip', '/usr/lib/python3.7', '/usr/lib/python3.7/lib-dynload', '/usr/local/lib/python3.7/dist-packages', '/usr/lib/python3/dist-packages', '/usr/local/lib/python3.7/dist-packages/IPython/extensions', '../', 'drive/Othercomputers/MacBookPro/', '../']\n","Using cuda\n","dataset to:  cpu\n","['val', 'train', 'test']\n","tcmalloc: large alloc 1551360000 bytes == 0x9488000 @  0x7fbf0820f001 0x7fbe97a841af 0x7fbe97adac23 0x7fbe97adba87 0x7fbe97b7d823 0x5936cc 0x548c51 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7fbf07e0ac87 0x5b621a\n","Loading val - 1 poses, images (800x800) and lights...\n","\tChanged 12 items out of 1212 in array of shape (101, 3, 4)\n","\t\tFrom (0, 0, 0) to (0, 2, 3)\n","Loading train - 100 poses, images (800x800) and lights...\n","\tChanged 1145 items out of 1212 in array of shape (101, 3, 4)\n","\t\tFrom (1, 0, 0) to (100, 2, 3)\n","Generating rays: 100% 101/101 [00:19<00:00,  5.18pose/s]\n","tcmalloc: large alloc 3102720000 bytes == 0x11eb02000 @  0x7fbf081efb6b 0x7fbf0820f379 0x7fbe9876cd57 0x7fbe9875abc3 0x7fbec26656af 0x7fbec2666020 0x7fbec2666074 0x7fbec26661bf 0x7fbec33ed82b 0x7fbec3450d42 0x7fbec2b90e37 0x7fbec33f689e 0x7fbec33f6923 0x7fbec2f07565 0x7fbec2ba3531 0x7fbec353c1a3 0x7fbec300e85c 0x7fbec452df19 0x7fbec452e416 0x7fbec3060132 0x7fbeea44c563 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1\n","Creating datasets...\n","\tComputing view dirs for val...\n","\tCreating val dataset with rays (torch.Size([640000, 9])) and images (torch.Size([640000, 3])) - shuffle True\n","tcmalloc: large alloc 3072000000 bytes == 0x7fbd53c3e000 @  0x7fbf081efb6b 0x7fbf0820f379 0x7fbe9876cd57 0x7fbe9875abc3 0x7fbec26656af 0x7fbec2666020 0x7fbec2666074 0x7fbec2b77def 0x7fbec33e9b8b 0x7fbec31351d3 0x7fbec33c4b8f 0x7fbec3172bf7 0x7fbec26a972c 0x7fbec26ab1cf 0x7fbec26acb83 0x7fbec2b3fe96 0x7fbec2b4cac9 0x7fbec33ea000 0x7fbec2ffb20c 0x7fbec4502e4d 0x7fbec4503503 0x7fbec304b05b 0x7fbeea5d3c62 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","tcmalloc: large alloc 1536000000 bytes == 0x687f8000 @  0x7fbf081efb6b 0x7fbf0820f379 0x7fbe9876cd57 0x7fbe9875abc3 0x7fbec26656af 0x7fbec2666020 0x7fbec2666074 0x7fbec2b77def 0x7fbec33e9b8b 0x7fbec31351d3 0x7fbec33c4b8f 0x7fbec3172bf7 0x7fbec26a972c 0x7fbec26ab1cf 0x7fbec26acb83 0x7fbec2b3fe96 0x7fbec2b4cac9 0x7fbec33ea000 0x7fbec2ffb20c 0x7fbec4502e4d 0x7fbec4503503 0x7fbec304b05b 0x7fbeea5d3c62 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","\tComputing view dirs for train...\n","tcmalloc: large alloc 4608000000 bytes == 0x7fbbe58de000 @  0x7fbf081efb6b 0x7fbf0820f379 0x7fbe9876cd57 0x7fbe9875abc3 0x7fbec26656af 0x7fbec2666020 0x7fbec2666074 0x7fbec26661bf 0x7fbec33ed82b 0x7fbec3450d42 0x7fbec2b90e37 0x7fbec33f689e 0x7fbec33f6923 0x7fbec2ecb49c 0x7fbec4445fe5 0x7fbec44467b6 0x7fbec2f07565 0x7fbeea46ec92 0x593835 0x548c51 0x5127f1 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173\n","\tCreating train dataset with rays (torch.Size([64000000, 9])) and images (torch.Size([64000000, 3])) - shuffle True\n","Datasets created successfully\n","\n","Reading PLY file: drive/Othercomputers/MacBookPro/microphone_light/meshed-poisson_aligned.ply[========================================] 100%\n","Generating ray lights: 100% 1/1 [00:00<00:00,  5.62pose/s]\n","Generating ray lights: 100% 100/100 [00:19<00:00,  5.25pose/s]\n","tcmalloc: large alloc 3072000000 bytes == 0x7fbb2c728000 @  0x7fbf081efb6b 0x7fbf0820f379 0x7fbe9876cd57 0x7fbe9875abc3 0x7fbec26656af 0x7fbec2666020 0x7fbec2666074 0x7fbec26661bf 0x7fbec33ed82b 0x7fbec3450d42 0x7fbec2b90e37 0x7fbec33f689e 0x7fbec33f6923 0x7fbec2f07565 0x7fbec2ba3531 0x7fbec353c1a3 0x7fbec300e85c 0x7fbec452df19 0x7fbec452e416 0x7fbec3060132 0x7fbeea44c563 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173\n","Running kmeans on cuda: 2it [00:08,  4.14s/it, center_shift=1.482074, num_clusters=511, tol=2.000000]\n","Filtered 55535448 points to 1644\n","Computing linear mappings: 100% 512/512 [00:54<00:00,  9.46linear mapping/s]\n","Training time: 62.81370782852173 seconds. Including 54.134477376937866 of K-means training.\n","Setting up [LPIPS] perceptual loss: trunk [vgg], v[0.1], spatial [off]\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.\n","  f\"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, \"\n","/usr/local/lib/python3.7/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n","  warnings.warn(msg)\n","Loading model from: /usr/local/lib/python3.7/dist-packages/lpips/weights/v0.1/vgg.pth\n","evaluating...\n","WARNING - 2022-08-19 10:03:17,378 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 10:03:17,399 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 10:03:17,421 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 10:03:17,444 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","Prediction time: 0.09454798698425293 seconds\n","WARNING - 2022-08-19 10:03:19,916 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 10:03:19,940 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 10:03:19,970 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 10:03:19,994 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"]}],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/reflectance_mapping.py -c drive/Othercomputers/MacBookPro/Rendering/configs/reflectancemap_colab.conf"]},{"cell_type":"markdown","source":["# Enhanced Reflectance Network"],"metadata":{"id":"2e66Hcgye53Y"}},{"cell_type":"code","source":["! python3 drive/Othercomputers/MacBookPro/Rendering/enhanced_reflectance_network.py -c drive/Othercomputers/MacBookPro/Rendering/configs/enhanced_reflectance_network.conf"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RJHQFi7He92n","outputId":"9bbba69a-f514-45dc-edd5-cb9556cfbace"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using cuda\n","dataset to:  cpu\n","['val', 'train', 'test']\n","Loading val - 1 poses, images (800x800) and lights...\n","\rLoading val frames: 0frame [00:00, ?frame/s]\r                                            \r\tChanged 12 items out of 132 in array of shape (11, 3, 4)\n","\t\tFrom (0, 0, 0) to (0, 2, 3)\n","Loading train - 10 poses, images (800x800) and lights...\n","\tChanged 115 items out of 132 in array of shape (11, 3, 4)\n","\t\tFrom (1, 0, 0) to (10, 2, 3)\n","Generating rays: 100% 11/11 [00:02<00:00,  5.07pose/s]\n","Creating datasets...\n","\tComputing view dirs for val...\n","\tCreating val dataset with rays (torch.Size([640000, 9])) and images (torch.Size([640000, 3])) - shuffle True\n","\tComputing view dirs for train...\n","\tCreating train dataset with rays (torch.Size([6400000, 9])) and images (torch.Size([6400000, 3])) - shuffle True\n","Datasets created successfully\n","\n","Reading PLY file: drive/Othercomputers/MacBookPro/microphone_light/meshed-poisson_aligned.ply[========================================] 100%\n","Generating ray lights: 100% 1/1 [00:00<00:00,  4.99pose/s]\n","Generating ray lights: 100% 10/10 [00:01<00:00,  5.61pose/s]\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mggarrofe\u001b[0m (\u001b[33mguillemgarrofe\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.13.1\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220819_112424-3nbwhr8q\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33molive-oath-700\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/guillemgarrofe/controllable-neural-rendering\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/guillemgarrofe/controllable-neural-rendering/runs/3nbwhr8q\u001b[0m\n","Running kmeans on cuda: 1it [00:00, 80.77it/s, center_shift=0.140912, num_clusters=7, tol=1.000000]\n","Computing linear mappings: 100% 8/8 [00:02<00:00,  3.29linear mapping/s]\n","  0% 0/100000 [00:00<?, ?iteration/s]WARNING - 2022-08-19 11:24:44,996 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 11:24:45,021 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 11:24:45,044 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 11:24:45,069 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 11:24:48,440 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 11:24:48,463 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 11:24:48,485 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 11:24:48,510 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 11:24:51,807 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 11:24:51,830 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 11:24:51,853 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 11:24:51,876 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 11:24:55,241 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 11:24:55,264 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 11:24:55,287 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-19 11:24:55,310 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","  0% 11/100000 [02:20<324:57:15, 11.70s/iteration]"]}]},{"cell_type":"markdown","metadata":{"id":"NNAwcSy0Bg3a"},"source":["# Residual Reflectance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NP64cGoHtdOl"},"outputs":[],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/residual_reflectance.py -c drive/Othercomputers/MacBookPro/Rendering/configs/resref_colab.conf"]},{"cell_type":"markdown","metadata":{"id":"vH5hJcZwjUiV"},"source":["# Reflectance Mapping Network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"06tpJrprjXhH"},"outputs":[],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/reflectance_mapping_network.py -c drive/Othercomputers/MacBookPro/Rendering/configs/reflectancemap_net_colab.conf"]},{"cell_type":"markdown","metadata":{"id":"Oyrqih7B6X03"},"source":["# Cluster selector network"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZWm3eBYmjHF5"},"outputs":[],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/reflectance_clusterized_network.py -c drive/Othercomputers/MacBookPro/Rendering/configs/reflectancemap_net_colab.conf"]},{"cell_type":"markdown","metadata":{"id":"FfQx7dI56o4I"},"source":["# Linear Mapping + Residual MLP"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true},"id":"e0o_N4q36we6"},"outputs":[],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/mapping_residual_reflectance.py -c drive/Othercomputers/MacBookPro/Rendering/configs/reflectancemap_net_colab.conf"]},{"cell_type":"markdown","metadata":{"id":"a0KW_rtpkGCj"},"source":["# Autodecoder"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KIjtKkOakJJG","outputId":"43c6ac7c-69cc-4770-e678-b8f0bc42a91f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda\n","['train', 'val']\n","tcmalloc: large alloc 1689600000 bytes == 0x9050000 @  0x7f8ef0804001 0x7f8e800f21af 0x7f8e80148c23 0x7f8e80149a87 0x7f8e801eb823 0x5936cc 0x548c51 0x51566f 0x549576 0x593fce 0x548ae9 0x5127f1 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173 0x5f5506 0x5f8c6c 0x5f9206 0x64faf2 0x64fc4e 0x7f8ef03ffc87 0x5b621a\n","Loading train - 100 poses, images (800x800) and lights...\n","\tChanged 1150 items out of 1320 in array of shape (110, 3, 4)\n","\t\tFrom (0, 0, 0) to (99, 2, 3)\n","Loading val - 10 poses, images (800x800) and lights...\n","\tChanged 115 items out of 1320 in array of shape (110, 3, 4)\n","\t\tFrom (100, 0, 0) to (109, 2, 3)\n","Generating rays: 100% 110/110 [00:20<00:00,  5.30pose/s]\n","tcmalloc: large alloc 3379200000 bytes == 0x7f8d2bd3a000 @  0x7f8ef07e4b6b 0x7f8ef0804379 0x7f8e80ddad57 0x7f8e80dc8bc3 0x7f8eaacbe39f 0x7f8eaacbed10 0x7f8eaacbed64 0x7f8eaacbeeaf 0x7f8eaba4453b 0x7f8eabaa7a52 0x7f8eab1e9047 0x7f8eaba4d5ae 0x7f8eaba4d633 0x7f8eab55e725 0x7f8eab1fb741 0x7f8eabb91983 0x7f8eab66590c 0x7f8eacb7db1d 0x7f8eacb7e026 0x7f8eab6b71d2 0x7f8ed2a82343 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1\n","Creating datasets...\n","tcmalloc: large alloc 3072000000 bytes == 0x6dba4000 @  0x7f8ef07e4b6b 0x7f8ef0804379 0x7f8e80ddad57 0x7f8e80dc8bc3 0x7f8eaacbe39f 0x7f8eaacbed10 0x7f8eaacbed64 0x7f8eab1cffff 0x7f8eaba4089b 0x7f8eab78c223 0x7f8eaba1b9bf 0x7f8eab7c9bf7 0x7f8eaad0232c 0x7f8eaad03dcf 0x7f8eaad05783 0x7f8eab1980a6 0x7f8eab1a4cd9 0x7f8eaba40d10 0x7f8eab6522bc 0x7f8eacb5287d 0x7f8eacb52f63 0x7f8eab6a20fb 0x7f8ed2c098e2 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","tcmalloc: large alloc 1536000000 bytes == 0x7f8ccfc62000 @  0x7f8ef07e4b6b 0x7f8ef0804379 0x7f8e80ddad57 0x7f8e80dc8bc3 0x7f8eaacbe39f 0x7f8eaacbed10 0x7f8eaacbed64 0x7f8eab1cffff 0x7f8eaba4089b 0x7f8eab78c223 0x7f8eaba1b9bf 0x7f8eab7c9bf7 0x7f8eaad0232c 0x7f8eaad03dcf 0x7f8eaad05783 0x7f8eab1980a6 0x7f8eab1a4cd9 0x7f8eaba40d10 0x7f8eab6522bc 0x7f8eacb5287d 0x7f8eacb52f63 0x7f8eab6a20fb 0x7f8ed2c098e2 0x4d3969 0x512147 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96\n","\tComputing view dirs for train...\n","tcmalloc: large alloc 4608000000 bytes == 0x7f8b24872000 @  0x7f8ef07e4b6b 0x7f8ef0804379 0x7f8e80ddad57 0x7f8e80dc8bc3 0x7f8eaacbe39f 0x7f8eaacbed10 0x7f8eaacbed64 0x7f8eaacbeeaf 0x7f8eaba4453b 0x7f8eabaa7a52 0x7f8eab1e9047 0x7f8eaba4d5ae 0x7f8eaba4d633 0x7f8eab52266c 0x7f8eaca96380 0x7f8eaca96b36 0x7f8eab55e725 0x7f8ed2aa4a62 0x593835 0x548c51 0x5127f1 0x549e0e 0x593fce 0x511e2c 0x4bc98a 0x532b86 0x594a96 0x548cc1 0x51566f 0x549576 0x604173\n","\tCreating train dataset with rays (torch.Size([64000000, 9])) and images (torch.Size([64000000, 3])) - shuffle True\n","\tComputing view dirs for val...\n","\tCreating val dataset with rays (torch.Size([6400000, 9])) and images (torch.Size([6400000, 3])) - shuffle True\n","Datasets created successfully\n","\n","Reading PLY file: drive/Othercomputers/MacBookPro/hotdog/hot-dogs2.ply[========================================] 100%\n","Generating ray lights: 100% 100/100 [00:18<00:00,  5.45pose/s]\n","tcmalloc: large alloc 3072000000 bytes == 0x7f8a6b6bc000 @  0x7f8ef07e4b6b 0x7f8ef0804379 0x7f8e80ddad57 0x7f8e80dc8bc3 0x7f8eaacbe39f 0x7f8eaacbed10 0x7f8eaacbed64 0x7f8eaacbeeaf 0x7f8eaba4453b 0x7f8eabaa7a52 0x7f8eab1e9047 0x7f8eaba4d5ae 0x7f8eaba4d633 0x7f8eab55e725 0x7f8eab1fb741 0x7f8eabb91983 0x7f8eab66590c 0x7f8eacb7db1d 0x7f8eacb7e026 0x7f8eab6b71d2 0x7f8ed2a82343 0x593784 0x548c51 0x51566f 0x549e0e 0x593fce 0x511e2c 0x593dd7 0x511e2c 0x549576 0x604173\n","Generating ray lights: 100% 10/10 [00:01<00:00,  5.85pose/s]\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mggarrofe\u001b[0m (\u001b[33mguillemgarrofe\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.12.21\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/wandb/run-20220802_175743-1i3azx8n\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmajor-violet-648\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/guillemgarrofe/controllable-neural-rendering\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/guillemgarrofe/controllable-neural-rendering/runs/1i3azx8n\u001b[0m\n","Running kmeans on cuda: 5it [00:38,  7.79s/it, center_shift=0.070143, num_clusters=249, tol=0.100000]\n","WARNING - 2022-08-02 17:58:41,062 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-02 17:58:41,084 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-02 17:58:42,530 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-02 17:58:42,552 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-02 17:58:43,808 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-02 17:58:43,829 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-02 17:58:44,971 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-02 17:58:44,993 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-02 18:00:26,674 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-02 18:00:26,697 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-02 18:00:27,970 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-02 18:00:27,992 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-02 18:00:29,209 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-02 18:00:29,230 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-02 18:00:30,375 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","WARNING - 2022-08-02 18:00:30,396 - image - Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n","  1% 17/2000 [03:07<5:39:01, 10.26s/epoch]"]}],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/reflectance_autodecoder.py -c drive/Othercomputers/MacBookPro/Rendering/configs/autodecoder_colab.conf"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SrmjZo4EAyNP"},"outputs":[],"source":["! python3 drive/Othercomputers/MacBookPro/Rendering/reflectance_autoencoder.py -c drive/Othercomputers/MacBookPro/Rendering/configs/autodecoder_colab.conf"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":206,"status":"ok","timestamp":1658579073545,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-60},"id":"bEKtpUsLkEDj","outputId":"12905460-5c0d-4e2a-e60d-73e4c572ab53"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[1, 2, 3],\n","        [4, 5, 6],\n","        [7, 8, 9]])\n","tensor([2, 1, 0, 1, 0])\n","tensor([0, 1, 4])\n"]}],"source":["import torch\n","t1 = torch.tensor([[1, 2, 3], [4, 5, 6], [1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","\n","unique, inverse = torch.unique(t1, sorted=True, return_inverse=True, dim=0)\n","perm = torch.arange(inverse.size(0), dtype=inverse.dtype, device=inverse.device)\n","inverse, perm = inverse.flip([0]), perm.flip([0])\n","print(unique)\n","print(inverse)\n","perm = inverse.new_empty(unique.size(0)).scatter_(0, inverse, perm)\n","print(perm)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1658932255976,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-60},"id":"PN0hRKzC-9je","outputId":"49e9c7f2-b8cd-4d30-a966-5f6e40c6cf38"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[-1.5066, -0.0317,  0.7404, -0.8176],\n","        [ 1.2103, -1.2459, -0.9263,  1.2217],\n","        [-0.0228, -1.3056,  0.7262,  0.2709],\n","        [-0.6637, -2.1896, -0.3487, -1.2596]])\n","tensor([[False, False,  True,  True],\n","        [False, False,  True,  True],\n","        [False, False,  True,  True],\n","        [False, False,  True,  True]])\n","tensor([2, 3, 2, 2])\n"]}],"source":["x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)\n","index = torch.tensor([0, 2])\n","x.index_fill_(0, index, -1)\n","\n","cluster_ids = []"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":401,"status":"ok","timestamp":1658937704446,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-60},"id":"bX9HgIyMo0Ye","outputId":"426065c6-d827-4cd3-8927-76cd7649537f"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([[9.2120e-01, 6.5819e-01, 4.7914e-01, 1.4890e-01, 5.2660e-01, 6.3940e-01,\n","         9.8772e-01, 5.9303e-01, 7.0703e-01, 8.4739e-01],\n","        [8.5110e-01, 4.0239e-04, 2.8202e-01, 9.1530e-01, 1.8869e-01, 4.2283e-01,\n","         2.7864e-01, 1.2488e-01, 4.7079e-01, 2.5965e-01],\n","        [6.6420e-01, 7.4319e-01, 7.1035e-01, 9.8808e-01, 5.7236e-01, 3.9414e-01,\n","         9.3400e-05, 8.8030e-01, 5.3952e-01, 1.6256e-01],\n","        [9.9138e-01, 9.0262e-01, 4.6338e-01, 7.3467e-01, 6.1284e-01, 4.9806e-01,\n","         5.6239e-01, 3.8199e-01, 4.3856e-01, 4.6844e-01],\n","        [9.2781e-01, 5.1247e-01, 4.5681e-01, 5.4670e-01, 2.5513e-01, 3.3994e-01,\n","         3.1610e-01, 4.8427e-01, 7.8315e-01, 8.5255e-02]])\n","tensor([[2, 0, 1, 4, 3],\n","        [3, 1, 2, 5, 4],\n","        [4, 2, 3, 6, 5]])\n","tensor([[4.7914e-01, 1.4890e-01, 5.2660e-01],\n","        [8.5110e-01, 4.0239e-04, 2.8202e-01],\n","        [7.4319e-01, 7.1035e-01, 9.8808e-01],\n","        [6.1284e-01, 4.9806e-01, 5.6239e-01],\n","        [5.4670e-01, 2.5513e-01, 3.3994e-01]])\n","torch.Size([5, 3])\n"]}],"source":["X = torch.tensor([[1., 2., 3.], [4., 5., 6.], [1, 2, 3], [4, 5, 6], [7, 8, 9]])\n","cluster_ids = torch.arange(0, end=5)\n","cluster_ids = cluster_ids[torch.randperm(cluster_ids.shape[0])]\n","\n","values = torch.rand((5, 10))\n","print(values)\n","col_indices = torch.stack([cluster_ids, cluster_ids+1, cluster_ids+2], dim=0)\n","print(col_indices)\n","row_indices = torch.arange(5)\n","print(values[row_indices, col_indices].T)\n","print(values[row_indices, col_indices].T.shape)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":234,"status":"ok","timestamp":1659060347075,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-60},"id":"w661fgkcleJB","outputId":"68409c9d-d9c6-4107-9e24-5360be758cdc"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([10, 15])\n","torch.Size([3, 15])\n"]},{"data":{"text/plain":["tensor([[ 0.5149, -0.2283,  0.1728,  0.2384, -0.1754, -0.0055,  0.0579,  0.5413,\n","         -0.4568, -0.0634,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n","        [-0.4487, -0.1163,  0.1923, -0.0186,  0.7851, -0.2504,  0.1239, -0.3273,\n","          0.5835, -0.1806,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n","        [ 0.1145,  0.5559, -0.1027, -0.0907, -0.4760,  0.4144,  0.0624, -0.0064,\n","          0.0639,  0.3303,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])"]},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"source":["import torch\n","cluster_ids = torch.arange(0,10)\n","\n","X = torch.rand((10, 3))\n","\n","def get_x2cluster(X, cluster_ids, num_clusters):\n","    indices = torch.stack([torch.arange(0,cluster_ids.shape[0]), cluster_ids])\n","    clusters = torch.sparse_coo_tensor(indices, torch.ones((cluster_ids.shape[0],)), (cluster_ids.shape[0], num_clusters)).to_dense()\n","    print(clusters.shape)\n","    x2cluster = torch.linalg.pinv(X) @ clusters  \n","    print(x2cluster.shape)\n","    return x2cluster\n","\n","get_x2cluster(X, cluster_ids, 15)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xlZSbBZyp4fF"},"outputs":[],"source":[""]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"run_nerf.ipynb","toc_visible":true,"provenance":[]},"kernelspec":{"display_name":"Python 3.9.13 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.13"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}