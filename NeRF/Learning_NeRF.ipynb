{"cells":[{"cell_type":"markdown","metadata":{"id":"9Fkyi6nnEISN"},"source":["# Setting up the working environment"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1651597022030,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-60},"id":"sNpdbKb21EWu"},"outputs":[],"source":["environment = \"COLAB\" # COLAB, SSH\n","debug = False"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1751,"status":"ok","timestamp":1651597023776,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-60},"id":"_UQ6828BDaJO","outputId":"36a88261-f956-49d4-992b-ca072187db67"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["if environment == \"COLAB\":\n","    from google.colab import drive\n","    import sys\n","    drive.mount('/content/drive')\n","    #sys.path.append('/content/drive/MyDrive/Colab Notebooks/Individual Project')\n","    sys.path.append('/content/drive/Othercomputers/MacBookPro/NeRF')\n","else: \n","    ! ls"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":416,"status":"ok","timestamp":1651597024188,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-60},"id":"6aOm7r4AD-6Z"},"outputs":[],"source":["if environment == \"COLAB\":\n","    ! cd 'drive/Othercomputers/MacBookPro/NeRF'"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1083,"status":"ok","timestamp":1651597025268,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-60},"id":"A5W-4iSCEpmg","outputId":"83c6759f-6d1c-4d51-89a3-d6a924904bcc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using cuda\n"]}],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from matplotlib.patches import Rectangle\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.utils.data import TensorDataset, DataLoader\n","from tqdm.notebook import tqdm\n","import seaborn as sn\n","import os\n","import visualization as v\n","import data\n","\n","\n","# We set a random seed to ensure that results are reproducible.\n","if torch.cuda.is_available():\n","    torch.backends.cudnn.deterministic = True\n","torch.manual_seed(0)\n","\n","GPU = True # Choose whether to use GPU\n","if GPU:\n","    device = torch.device(\"cuda\"  if torch.cuda.is_available() else \"cpu\")\n","else:\n","    device = torch.device(\"cpu\")\n","print(f'Using {device}')"]},{"cell_type":"markdown","metadata":{"id":"PcJL2XpqHl6R"},"source":["## Download the data"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1651597025270,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-60},"id":"pl9hcSXZKASC"},"outputs":[],"source":["if not os.path.exists('tiny_nerf_data.npz'):\n","    !wget https://people.eecs.berkeley.edu/~bmild/nerf/tiny_nerf_data.npz"]},{"cell_type":"markdown","metadata":{"id":"BfgGAHbvEw7q"},"source":["# Understanding the Data\n","\n","The input of the MLP network that will conform the Neural Radiance Field Scene Representation is meant to be a 3D location $\\mathbf{x} = (x,y,z)$ and a 2D viewing direction $(\\theta, \\phi)$. \n","\n","\n","<p align=\"center\">\n","<img src=\"https://drive.google.com/uc?export=view&id=10OuT4HyclVgcAoR7u7j8MMOI1aw4o3a0\" alt=\"NeRF MLP\" width=\"400\"/>\n","</p>\n","<p align=\"center\">\n","<b>NeRF MLP [source: <a href=\"https://www.matthewtancik.com/nerf\">NeRF Page</a>]</b>\n","</p>\n","\n","However, the available data in the dataset consists of images, camera poses, and a focal length. Therefore, **we will have to convert each pixel of the images to a 3D location and a 2D viewing direction with the camera pose and the corresponding focal length**.\n","\n","## Camera projection\n","\n","First of all, we need to understand that the image is a projection of the real world to the image frame. Specifically, the projection can be splitted in different transformations: \n","$$\n","\\begin{aligned}\n","world\\ coords\\ [3D] → camera\\ coords\\ [3D] → image\\ coords\\ (image\\ plane)\\ [2D] → pixel\\ coordinates\\ [2D].\n","\\end{aligned}\n","$$\n","\n","1. We can express the **world coordinates in the camera frame** if we have the transformation between the world frame and the camera frame $T_{c \\rightarrow w}$. This transformation consists in a Rigid Transformation (rotation + translation) between world and camera coordinate systems.\n","\n","\n","$$\n","\\begin{aligned}\n","{}^{c}P = T_{c \\rightarrow w}\\ {}^{w}P \\hspace{2cm} \\begin{bmatrix}\n","{}^{c}X\\\\\n","{}^{c}Y\\\\\n","{}^{c}Z\n","\\end{bmatrix} =  \\mathbf{R} \\begin{bmatrix}\n","{}^{w}X\\\\\n","{}^{w}Y\\\\\n","{}^{w}Z\\\\\n","\\end{bmatrix} + \\mathbf{t} \\hspace{2cm} \\begin{bmatrix}\n","{}^{c}X\\\\\n","{}^{c}Y\\\\\n","{}^{c}Z\\\\\n","1\n","\\end{bmatrix} = \\begin{bmatrix}\n","\\mathbf{R} & \\mathbf{t}\\\\\n","0 & 1\n","\\end{bmatrix} \\begin{bmatrix}\n","{}^{w}X\\\\\n","{}^{w}Y\\\\\n","{}^{w}Z\\\\\n","1\n","\\end{bmatrix} \\hspace{2cm} \\begin{bmatrix}\n","{}^{c}X\\\\\n","{}^{c}Y\\\\\n","{}^{c}Z\\\\\n","1\n","\\end{bmatrix} = \\begin{bmatrix}\n","r_{11} & r_{12} & r_{13} & t_{x}\\\\\n","r_{21} & r_{22} & r_{23} & t_{y}\\\\\n","r_{31} & r_{32} & r_{33} & t_{z}\\\\\n","0 & 0 & 0 & 1\n","\\end{bmatrix} \\begin{bmatrix}\n","{}^{w}X\\\\\n","{}^{w}Y\\\\\n","{}^{w}Z\\\\\n","1\n","\\end{bmatrix} \n","\\end{aligned}\n","$$\n","\n","<p align=\"center\">\n","<img src=\"https://drive.google.com/uc?export=view&id=1W8J1905I7xJxds5GlOGr65uqwEH73z3P\" alt=\"world2cam\" width=\"400\"/>\n","</p>\n","\n","<p align=\"center\">\n","<b>World frame to camera frame [source: adaptated from the Imperial College Computer Vision course' slides]</b>\n","</p>\n","\n","<ul>\n","Fortunately, the transformation matrix is contained within the camera poses and we can transform the coordinates from the world frame to the camera frame and more importantly from the camera frame to the world frame by using the inverse. In this case, we will have to use the inverse of the rigit transformation, which is also a rigid transformation:\n","</ul>\n","\n","$$\n","\\begin{aligned}\n","T^{-1}_{(R, t)} = T_{(R^{T}, -R^{T} t)}\n","\\end{aligned}\n","$$\n","\n","2. Once the elements are expressed in the camera frame, we can project them in the image plane. We can do so by using Thales' theorem of similar triangles:\n","\n","$$\n","\\begin{aligned}\n","x = f \\frac{{}^{c}X}{{}^{c}Z} \\hspace{2cm} y = f \\frac{{}^{c}Y}{{}^{c}Z} \n","\\end{aligned}\n","$$\n","\n","\n","<p align=\"center\">\n","<img src=\"https://drive.google.com/uc?export=view&id=1-L2w4Kyf2o9MTu3dh3VK6PCQBr0IRt4G\" alt=\"cam2img\" width=\"600\"/>\n","</p>\n","\n","<p align=\"center\">\n","<b>Projection to the image frame [source: adaptated from the Imperial College Computer Vision course' slides]</b>\n","</p>\n","\n","<ul>Note that for any point $(x, y)$ in the Euclidean plane its representation in the projective plane is simply $(x, y, 1)$. Moreover, we can represent the same point in homogeneous coordinates by adding a third coordinate with a 3D point $(x', y', z')$ such as:</ul>\n","\n","$$\n","\\begin{aligned}\n","x = \\frac{x'}{z'} \\hspace{2cm} y = \\frac{y'}{z'} \\hspace{2cm} \\begin{bmatrix} x'\\\\ y'\\\\ z'\\end{bmatrix} = \\begin{bmatrix} f & 0 & 0 & 0\\\\\n","0 & f & 0 & 0\\\\\n","0 & 0 & 1 & 0\\end{bmatrix} \\begin{bmatrix} {}^{c}X\\\\ {}^{c}Y\\\\ {}^{c}Z\\\\ 1 \\end{bmatrix}\n","\\end{aligned}\n","$$\n","\n","3. The image coordinates above are not expressed in pixel coordinates yet. To convert to pixel coordinates we are not going to consider the center of the image $(x_0, y_0)$ as the origin of our coordinate system. We will add the center of the image to move the origint ot the top-left corner:\n","\n","$$\n","\\begin{aligned}\n","u = x + x_0 \\hspace{2cm} v = y + y_0\n","\\end{aligned}\n","$$\n","\n","<p align=\"center\">\n","<img src=\"https://drive.google.com/uc?export=view&id=1IjlQqVSv7wjLtTeTJH49AKZpuoOTsORR\" alt=\"cam2img\" width=\"400\"/>\n","</p>\n","\n","<p align=\"center\">\n","<b>Image plane coords to pixel coords [source: authors' image]</b>\n","</p>\n","\n","<ul>This can be further expressed as:</ul>\n","\n","$$\n","\\begin{aligned}\n","\\begin{bmatrix} u'\\\\ v'\\\\ w'\\end{bmatrix} = \\begin{bmatrix} {}^{c}Z\\ u\\\\ {}^{c}Z\\ v\\\\ {}^{c}Z\\end{bmatrix} = \\begin{bmatrix} f & 0 & x_0 & 0\\\\\n","0 & f & y_0 & 0\\\\\n","0 & 0 & 1 & 0\\end{bmatrix} \\begin{bmatrix} {}^{c}X\\\\ {}^{c}Y\\\\ {}^{c}Z\\\\ 1 \\end{bmatrix}\n","\\end{aligned}\n","$$\n","\n","<ul>And the matrix $C_{in} = \\begin{bmatrix} f & 0 & x_0 & 0\\\\\n","0 & f & y_0 & 0\\\\\n","0 & 0 & 1 & 0\\end{bmatrix}$ is called camera intrisics as it represents internal properties of the camera such as the focal length and the center of the image plane.</ul> \n"]},{"cell_type":"markdown","metadata":{"id":"PUWmmJy2QfTz"},"source":["## NeRF Dataset\n","\n","The data available in the NeRF Dataset consists of the different images captured from different points of view with its corresponding camera extrinsics and intrinsics. \n","\n","There are different datasets with different formats: \n","\n","### [Tiny NeRF](https://people.eecs.berkeley.edu/~bmild/nerf/tiny_nerf_data.npz)\n","\n","\n","The camera extrinsics are given within the *pose* matrix (```data[\"poses\"]```). Each *pose* consists of the transformation matrix $\\begin{bmatrix}\\mathbf{R} & \\mathbf{t}\\end{bmatrix}$ where the left 3x3 matrix is the rotation matrix $\\mathbf{R}$ and the transformation matrix $\\mathbf{t}$ as the right 3x1 matrix.\n","\n","We can infer the image height and width from the images (```data[\"images\"]```) itself and the focal length is given as an additional field of the data dictionary (i.e., ```data[\"focal\"]```). \n","\n","### [NeRF synthetic](https://drive.google.com/drive/folders/1JDdLGDruGNXWnM1eqY1FNL9PlStjaKWi)\n","\n","The transformation matrix is given for each frame, and the image height and width can also be infered from the images. To obtain the focal length, we can use the horizontal field of view (```camera_angle_x```) with the following equation:\n","\n","$$\n","\\begin{aligned}\n","focal = \\frac{image\\_width}{2\\ tan(\\frac{camera\\_angle\\_x}{2})} \n","\\end{aligned}\n","$$\n","\n","### [NeRF LLFF](https://drive.google.com/drive/folders/14boI-o5hGO9srnWaaogTU5_ji7wkX2S7)\n","\n","The camera extrinsics are also given within the *pose* matrix. In this case each *pose* has $\\begin{bmatrix}\\mathbf{R} & \\mathbf{t}\\end{bmatrix}$ as the left 3x4 matrix and ```[image height, image width, focal length]``` as the right 3x1 matrix.\n","\n","\\\\\n","\n","It is worth noting that in all the datasets, the first 3x3 block in the *pose* matrix (i.e., the rotation matrix $\\mathbf{R}$ from the Transformation matrix) corresponds to the camera’s point of view. In comparison with other frameworks, the $\\mathbf{R}$ matrix is in the form ```[down, right, back]``` or ```[-y, x, z]``` and the camera is pointing torwards ```-z```, instead of ```[right up back]``` as in OpenGL format. However, NeRF uses the same pose coordinate system as in OpenGL which is the reason why we must fix it after loading, note the NeRF dataloader's transformation in the image below, and the signs of the coordinates in the following line of code:\n","\n","```\n","directions = torch.stack([xx, -yy, -torch.ones_like(xx)], dim=-1) \n","```\n","\n","<p align=\"center\">\n","<img src=\"https://drive.google.com/uc?export=view&id=19BZNn5kJgcHiDjR85OruDSXiGAIjw9EY\" alt=\"cam2img\" width=\"600\"/>\n","</p>\n","\n","<p align=\"center\">\n","<b>Different coordinate frames used by different frameworks [source: @ziruiwang_ox twitter]</b>\n","</p>\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":18,"status":"ok","timestamp":1651597025270,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-60},"id":"ngoYHYfTHXvk"},"outputs":[],"source":["if debug:\n","    # Load input images, poses, and intrinsics\n","    data = np.load(\"tiny_nerf_data.npz\")\n","\n","    # Images - shape: (num_images, height, width, channels)\n","    images = data[\"images\"]\n","    num_images, height, width, num_channels = images.shape\n","\n","    # Camera extrinsics\n","    tform_cam2world = data[\"poses\"]\n","    tform_cam2world = torch.from_numpy(tform_cam2world).to(device)\n","\n","    # Focal length (intrinsics)\n","    focal_length = data[\"focal\"]\n","    focal_length = torch.from_numpy(focal_length).to(device)\n","\n","    hwf = (height, width, focal_length)\n","\n","    # Random example\n","    i_img = np.random.randint(low=0, high=num_images)\n","    plt.imshow(images[i_img])\n","    plt.show()\n","\n","    print(f\"\\nFocal length: {focal_length}\")\n","    print(f\"\\nTransformation matrix [R t]: \\n{tform_cam2world[i_img,:3,:4]}\")\n","    print(f\"\\nImage height, Image width, Focal length [H W F]: \\n{(height, width, focal_length)}\")"]},{"cell_type":"markdown","metadata":{"id":"JWh9xsqTQtGM"},"source":["## Getting the rays\n","\n","The process above consists of the forward projection (i.e., passing from the 3D world coordingates to the 2D image's pixels). However, we need to do the inverse process:\n","\n","1. Pass from pixel coordinates to image plane. Given the $height$ and the $width$ we know that $(x_0, y_0)$ is going to be $(\\frac{width}{2}, \\frac{height}{2})$ so we can pass the pixel coordinates to the image coordinates (i.e. the coordinates of the projection in the image plane) as follows:\n","\n","$$\n","\\begin{aligned}\n","x = u - \\frac{width}{2} \\hspace{2cm} y = v - \\frac{height}{2}\n","\\end{aligned}\n","$$\n","\n","2. From the coordinates in the image plane we will get the 3D coordinates in the camera frame. From before we now:\n","\n","$$\n","\\begin{aligned}\n","x = f \\frac{{}^{c}X}{{}^{c}Z} \\hspace{2cm} y = f \\frac{{}^{c}Y}{{}^{c}Z}\n","\\end{aligned}\n","$$\n","\n","<ul>So, now, we can get the 3D coordinates in the camera frame as follows:</ul>\n","\n","$$\n","\\begin{aligned}\n","{}^{c}X = \\frac{x\\ {}^{c}Z}{f} \\hspace{2cm} {}^{c}Y = \\frac{y\\ {}^{c}Z}{f}\n","\\end{aligned}\n","$$\n","\n","<ul>Therefore, we can pass from pixel coordinates to 3D coordinates in the camera frame, with the following equation: </ul>\n","\n","$$\n","\\begin{aligned}\n","{}^{c}X = \\frac{(u - \\frac{width}{2})\\ {}^{c}Z}{f} \\hspace{2cm} {}^{c}Y = \\frac{(v - \\frac{height}{2})\\ {}^{c}Z}{f}\n","\\end{aligned}\n","$$\n","\n","❓ Why ${}^{c}Z$ is considered to be 1 in NeRF❓ Is it because there is a convention in ray-tracing where the objects in the camera frame are placed exactly 1 unit away from the camera's origin❓\n","\n","3. Finally, we need to transform this coordinates to the world frame and we will get the directions of the rays. Considering that the camera origin in the camera frame was considered to be in the $(0,0,0)$ coordinate, the origin of the rays will be just the translation from the camera frame to the world frame.\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1651597025271,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-60},"id":"VFYEGcMeErOl"},"outputs":[],"source":["def get_rays_origins_and_directions(height, width, focal_length, pose):\n","\n","    rotation = pose[:3, :3]\n","    translation = pose[:3, -1]\n","\n","    # Obtain each pixel coordinates (u, v)\n","    uu, vv = torch.meshgrid(torch.arange(width), torch.arange(height), indexing='xy')\n","    uu, vv = uu.to(focal_length).transpose(-1, -2), vv.to(focal_length).transpose(-1, -2)\n","\n","    # Pass from pixel coordinates to image plane (uu - width * .5) and (vv - height * .5)\n","    # and from image plane to 3D coordinates in the camera frame (/focal_length)\n","    xx = (uu - width * .5) / focal_length\n","    yy = (vv - height * .5) / focal_length\n","\n","    # For any point (x, y) in the Euclidean plane its representation in the \n","    # projective plane is simply (x, y, 1).\n","    # \n","    # R matrix is in the form [down right back] instead of [right up back] \n","    # which is why we must fix it.\n","    #\n","    # Given the assumptions above, we are going to create, for each pixel\n","    # its corresponding direction vector considering the camera's point of \n","    # view (i.e., the R matrix)\n","\n","    directions = torch.stack([xx, -yy, -torch.ones_like(xx)], dim=-1) \n","    ray_directions = torch.sum(directions[..., None, :] * rotation, dim=-1)\n","    \n","    ray_origins = translation.expand(ray_directions.shape)\n","    return ray_origins, ray_directions"]},{"cell_type":"markdown","metadata":{"id":"6Y0895NGGSXU"},"source":["## Sampling 3D points\n","\n","Although it is inefficient to not use hierarchical sampling, for the sake of learning to perform volume sampling we will start by creating $N$ query points along each camera ray. We will use stratified sampling to get the different $N$ locations. Specificaly, from a minimum distance to the origin in the direction of the ray to a maximum distance in the same direction, we will create homogeneous divisions and then we will apply random sampling within each division. \n","\n","<p align=\"center\">\n","<img src=\"https://drive.google.com/uc?export=view&id=1ZBwWTsoQvdMcXhmNr8Xv_6r_7V50SSz2\" alt=\"cam2img\" width=\"600\"/>\n","</p>\n","<p align=\"center\">\n","<b>Stratified ray sampling [source: <a href=\"https://keras.io/examples/vision/nerf/\">keras</a>]</b>\n","</p>"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1651597025272,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-60},"id":"wbxVl0YHH6JD"},"outputs":[],"source":["def get_sampling_locations(rays_o, rays_d, near, far, n_samples, stratified=True):\n","    z_locations = torch.linspace(near, far, n_samples).to(rays_d)\n","    \n","    if stratified:\n","        # Generate for each ray, n_samples of uniform noise\n","        noise = torch.rand(list(rays_d.shape[:-1]) + [n_samples]).to(rays_d)\n","        \n","        # Limit the noise values from 0 to the length of the division\n","        noise *= (far-near)/n_samples\n","\n","        z_locations = z_locations + noise\n","\n","    # Need to broadcast the ray direction to each location\n","    locations = rays_o[...,None,:] + rays_d[...,None,:] * z_locations[...,:,None]\n","\n","    locations = torch.reshape(locations, [-1,n_samples,3])\n","    z_locations = torch.reshape(z_locations, [-1,n_samples])\n","    \n","    return locations, z_locations\n","\n","if debug:\n","    N_samples = 32\n","    rays_o, rays_d = get_rays_origins_and_directions(height, width, focal_length, tform_cam2world[0])\n","\n","    rays_o_flat = torch.reshape(rays_o, [-1, 3])\n","    rays_d_flat = torch.reshape(rays_d, [-1, 3])\n","    locations_flat, z_locations_flat = get_sampling_locations(rays_o_flat, rays_d_flat, near=2, far=6, n_samples=N_samples)\n","\n","    view_dirs = rays_d_flat\n","    view_dirs = view_dirs / torch.norm(view_dirs, dim=-1, keepdim=True)"]},{"cell_type":"markdown","metadata":{"id":"c5m_DQH75HTd"},"source":["# Optimizing a Neural Radiance Field (I)"]},{"cell_type":"markdown","metadata":{"id":"ih_6vjb75PdU"},"source":["\n","## Positional Encoding\n","\n","Mapping the inputs (i.e., 3D coordinates and viewing directions) to a higher-dimensional space before passing them to the neural network allows approximating easily a higher frequency function. Hence, the radiance field would be able to contain higher frequency variation both in color and geometry.\n","\n","Therefore, we will map the input data from $\\mathbb{R}$ into a higger dimensional space $\\mathbb{R}^{2L}$.\n","\n","To do so, we will use different trigonometric functions with different periods/frequencies (one for each dimension), as following:\n","\n","$$\n","\\gamma(p) = (sin (2^0 \\pi p), cos (2^0 \\pi p), \\dots , sin (2^{L-1} \\pi p), cos (2^{L-1} \\pi p))\n","$$\n","\n","This encoding is applied separately to each of the three coordinate values in x (which are normalized to lie in [−1, 1]) and to the three components of the Cartesian viewing direction unit vector $\\mathbf{d}$ (which by construction lie in [−1, 1]). In the original paper, they set $L = 10$ for $\\gamma(x)$ and $L = 4$ for $\\gamma(\\mathbf{d})$.\n","\n","The spacing between the frequencies can be both linear or logarithmic.\n"]},{"cell_type":"code","execution_count":9,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1651597025272,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-60},"id":"AoLy-ZQz5QZt"},"outputs":[],"source":["def positional_encoding(input, L=6, log_sampling=False):\n","\n","    if log_sampling:\n","        freq_bands = 2.**torch.linspace(0., L-1, L)\n","    else:\n","        freq_bands = torch.linspace(2.**0., 2.**(L-1), L)\n","\n","    enc = [input]\n","    for f in freq_bands:\n","        for fn in [torch.sin, torch.cos]:\n","            enc.append(fn(f * input))\n","            \n","    return torch.cat(enc, dim=-1)"]},{"cell_type":"markdown","metadata":{"id":"0dpxKh7eMRLy"},"source":["# Creating the model\n","\n","Refering to the [paper](https://arxiv.org/abs/2003.08934) and its [supplementary material](https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460392-supp.pdf) we know that the MLP first processes the input 3D coordinate $\\mathbf{x}$ with 8 fully-connected layers, using ReLU activations and 256 channels per layer. Then, it outputs the volume density $\\sigma$ and a 256-dimensional feature vector.\n","\n","To obtain the view-dependent RGB radiance, the feature vector is concatenated with the camera ray's viewing direction and passed to an andditional fully-connected layer using 128 channels and ReLU activation.\n","\n","A final layer with a sigmoid activation, outputs the emitted RGB radiance at position $\\mathbf{x}$, as viewed by a ray with direction $\\mathbf{d}$.\n","\n","Inspired by the DeepSDF architecture, the authors also suggest including a skip connection that concatenates this input to the fifth layer’s activation.\n","\n","<p align=\"center\">\n","<img src=\"https://drive.google.com/uc?export=view&id=1JLPSqw9mHtj-hVZrF8ZBRom_nIoBqKsM\" alt=\"cam2img\" width=\"600\"/>\n","</p>\n","\n","\n","<p align=\"center\">\n","<b> NeRF architecture [source: <a href=\"https://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460392-supp.pdf\">NeRF paper supplementary material</a>]</b>\n","</p>\n"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":15,"status":"ok","timestamp":1651597025274,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-60},"id":"ll8EqLnshgE_"},"outputs":[],"source":["class NeRFModel(nn.Module):\n","\n","    def __init__(self, D=8, W=256, input_ch=3, view_dir_ch=3, output_ch=4, \n","                 skips=[4], viewing_direction=True, L_x=10, L_d=4, pos_enc=True):\n","        super(NeRFModel, self).__init__()\n","\n","        self.input_ch = input_ch # size of vector x - usually 3D vector but it can have positional encoding\n","        self.view_dir_ch = view_dir_ch # we express the 2D viewing direction $(\\theta, \\phi)$ as a 3D Cartesian unit vector d\n","        self.output_ch = output_ch # 3 dimensions for the color value/radiance, 1 dimension for the volume density\n","        self.viewing_direction = viewing_direction\n","        self.skips = skips\n","        self.pos_enc = pos_enc\n","\n","        if not pos_enc:\n","            L_x = L_d = 0\n","\n","        self.L_x = L_x\n","        self.L_d = L_d\n","\n","        volume_density_layers = [nn.Linear(in_features=input_ch + input_ch*2*L_x, out_features=W)]\n","\n","        for l in range(1, D):\n","            volume_density_layers.append(nn.ReLU())\n","\n","            if l in skips:\n","                volume_density_layers.append(nn.Linear(in_features=W + input_ch + input_ch*2*L_x, out_features=W))\n","            else:\n","                volume_density_layers.append(nn.Linear(in_features=W, out_features=W))\n","\n","        if viewing_direction:\n","            self.volume_density_out = nn.Sequential(nn.Linear(in_features=W, out_features=1),\n","                                                    nn.ReLU())\n","            self.feature_vector = nn.Linear(in_features=W, out_features=W)\n","\n","            radiance_layers = [nn.Linear(in_features=W + view_dir_ch + view_dir_ch*2*L_d, out_features=W//2),\n","                               nn.Linear(in_features=W//2, out_features=3),\n","                               nn.Sigmoid()]\n","            self.radiance_layers = nn.Sequential(*radiance_layers)\n","\n","        else:\n","            volume_density_layers.append(nn.Linear(in_features=W, out_features=output_ch))\n","\n","        self.volume_density_layers = nn.Sequential(*volume_density_layers)\n","    \n","    def forward(self, x, view_dirs=None):\n","        enc_x = positional_encoding(x, L=self.L_x)\n","        out_density = enc_x\n","\n","        for l, layer in enumerate(self.volume_density_layers):\n","            out_density = layer(torch.cat((out_density, enc_x), dim=-1) if l/2 in self.skips else out_density)\n","            \n","        if self.viewing_direction:\n","            volume_density = self.volume_density_out(out_density)\n","            feature_vector = self.feature_vector(out_density)\n","            enc_view_dirs = positional_encoding(view_dirs, L=self.L_d)\n","            out_radiance = self.radiance_layers(torch.cat((feature_vector, enc_view_dirs), dim=-1))\n","            \n","        else:\n","            volume_density = torch.relu(out_density[..., 3])\n","            out_radiance = torch.sigmoid(out_density[..., :3])\n","            \n","        return torch.cat((out_radiance, volume_density), dim=-1)"]},{"cell_type":"markdown","metadata":{"id":"4zt0ju9GEuBk"},"source":["# Volume rendering\n","\n","We want to compute the expected color $C(\\mathbf{r})$ of camera ray with near and far bounds ($t_n$ and $t_f$):\n","\n","$$\n","C(\\mathbf{r}) = \\int^{t_f}_{t_n} T(t)\\sigma(\\mathbf{r}(t))\\mathbf{c}(\\mathbf{r}(t), \\mathbf{d}) dt\n","$$\n","\n","where:\n","\n","- $\\mathbf{r}(t)=\\mathbf{o}+t\\mathbf{d}$, with camera $\\mathbf{o}$, bounded in $[t_n, t_f]$\n","- $T(t)$ the accumulated transmittance along the ray from $t_n$ to $t$\n","    - probability that the ray travels from $t_n$ to $t$ without hitting any other particle\n","- $\\sigma(\\mathbf{r}(t))$ and $\\mathbf{c}(\\mathbf{r}(t), \\mathbf{d})$ are given by the MLP network $F_{\\theta}: (\\mathbf{x}, \\mathbf{d})$ → $(\\mathbf{c}, \\sigma)$\n","    - $\\sigma$ → differential probability of a ray terminating at an infinitesimal particle at location $\\mathbf{x}$\n","\n","We cannot compute the continuous integral, and that is the reason why we use stratified sampling when obtaining the rays' 3D locations.\n","\n","The expected color estimation is then computed as:\n","\n","$$\n","\\hat{C}(\\mathbf{r}) = \\sum_{i=1}^{N}T_i(1-e^{-\\sigma_i\\delta_i})\\ \\mathbf{c}_i\n","$$\n","\n","where:\n","\n","- $T_i = e^{-\\sum_{j=1}^{i-1} \\sigma_j \\delta_j}  = \\prod_{j=1}^{i-1} e^{-\\sigma_j \\delta_j}$\n","- $\\delta_i=t_{i+1}-t_i$, i.e., distance between adjacent samples\n","\n","Note that this equation can be understood as a weigthed sum of the sampled colors along the ray:\n","\n","$$\n","\\hat{C}(\\mathbf{r}) = \\sum_{i=1}^{N} w_i\\ \\mathbf{c}_i \\hspace{1cm} w_i = T_i(1-e^{-\\sigma_i\\delta_i})\n","$$\n","\n","This weights give an idea of the relevance of each part of the ray regarding the volume information that it reveals. The idea of hierarchical volume sampling relies in taking profit of this weights as it is exposed in the following section."]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1651597025275,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-60},"id":"frUm6m5aokTc"},"outputs":[],"source":["def volume_rendering(raw_radiance_density, z_locations, rays_d, raw_noise_std=0.):\n","\n","    # distance between adjacent samples\n","    dists = z_locations[..., 1:] - z_locations[..., :-1]\n","\n","    # last sample' distance is going to be infinity, we represent such distance \n","    # with a symbolic high value (i.e., 1e10)\n","    inf = torch.tensor([1e10]).to(dists)\n","    dists = torch.concat([dists, torch.broadcast_to(inf, dists[..., :1].shape)], dim=-1)\n","\n","    # convert the distances in the rays to real-world distances by multiplying \n","    # by the norm of the ray's direction\n","    dists = dists * torch.norm(rays_d[..., None, :], dim=-1)\n","\n","    # Adding noise to model's predictions for density helps regularizing the network \n","    # during training.\n","    raw_density = raw_radiance_density[..., 3]\n","    if raw_noise_std > 0.:\n","        noise = torch.randn(raw_density.shape).to(raw_density) * raw_noise_std\n","        raw_density = raw_density + noise\n","\n","    alpha = 1.0 - torch.exp(-raw_density * dists)\n","\n","    transmitance = torch.cumprod(1.0 - alpha + 1e-10, dim=-1)\n","\n","    # exclusive = True, for the first sample the transmitance should be 1 as the\n","    # probability to travel from one sample to the same sample without hitting any\n","    # particle is 1\n","    transmitance = torch.roll(transmitance, 1, -1)\n","    transmitance[..., 0] = 1.0\n","\n","    weights = transmitance * alpha\n","    rgb = torch.sum(weights[..., None] * raw_radiance_density[..., :3], dim=-2)\n","    return rgb, weights\n","    \n","if debug:\n","    view_dirs_flat = torch.broadcast_to(view_dirs[..., None, :], locations_flat.shape)\n","\n","    model = NeRFModel()\n","    model.to(device)\n","\n","    raw_radiance_density_flat = model(locations_flat, view_dirs=view_dirs_flat)\n","\n","    rgb_map, weights = volume_rendering(raw_radiance_density_flat, z_locations_flat, torch.reshape(rays_d, [-1, 3])) \n","    rgb_map = torch.reshape(rgb_map, rays_d.shape)\n","\n","    plt.figure(figsize=(10, 4))\n","    plt.imshow(rgb_map.detach().cpu().numpy())\n","    plt.show()"]},{"cell_type":"markdown","metadata":{"id":"rTXQqBKojLW8"},"source":["# Optimizing a Neural Radiance Field (II)"]},{"cell_type":"markdown","metadata":{"id":"rGa0Y9k9aqYy"},"source":["## Hierarchical volume sampling\n","\n","Instead of sampling 3D points along all the ray, we can try to focus on those parts of the ray that will give us relevant information of the object's volume.\n","\n","To do so, we will take advantage of the network's output, specifically the volume density $\\sigma(\\mathbf{x})$ that represents the differential probability of a ray terminating at an infinitesimal particle at location $\\mathbf{x}$. Specifically, if we focus on the way that we render the colors of the camera rays, we can understand such formula as a way of weighting the different colors across the ray. Hence, such weights give us information of the regions that effectively have volume and thus contribut to the expected effect on the final rendering.\n","\n","$$\n","\\hat{C}(\\mathbf{r}) = \\sum_{i=1}^{N} w_i\\ \\mathbf{c}_i \\hspace{1cm} w_i = T_i(1-e^{-\\sigma_i\\delta_i})\n","$$\n","\n","If we normalize such weights, we end up with a Probability Density Function (PDF) along the ray which reflects the relevance of each depth in the final rendering. \n","\n","$$\n","\\hat{w}_i = \\frac{w_i}{\\sum^{N_c}_{j=1} w_j}\n","$$\n","\n","Now, we can sample a second set of locations from this PDF, using inverse transform sampling, and evaluate this new points together with the previous ones (i.e., the ones obtaines with stratified sampling) with the *fine* network. \n","\n","The inverse transform sampling can be performed as following:\n","* Compute the Cummulative Distribution Function of the obtained PDF, i.e., the integration of the PDF. A cummulative distribution $F_X(x)$ is the probability that a random variable $X$ will have a lower value than $x$: $F_X(x) = P(X \\leq x)$ \n","* Generate a random number according to a uniform function $U \\sim Unif(0,1)$\n","* Determine the index $j$ such that $\\sum_{i=1}^{j-1} p_i \\leq U < \\sum_{i=1}^j p_i$. In our case, $p_i$ will be the weights of each location $\\hat{w}_i$. \n","* Take the sample from the location in the index $j$. Hence, the more weight a location has, the more chances it has to be picked in the second set of samples. \n","\n","**Actually, we are not going to take exactly the same location as the one associated to the weight $j$**, but one between location $j$ and location $j-1$. The interpolation is going to be made according to the difference between the random value $U$ and the cumulative probability of $j-1$ and $j$ (i.e., $\\sum_{i=1}^{j-1} \\hat{w}_i$ and $\\sum_{i=1}^j \\hat{w}_i$) such that:\n","\n","$$\n","location = location_{j-1} + \\frac{U - \\sum_{i=1}^{j-1} \\hat{w}_i}{\\sum_{i=1}^{j} \\hat{w}_i - \\sum_{i=1}^{j-1} \\hat{w}_i} * (location_j - location_{j-1})\n","$$\n","\n","This process is expected to allocate more samples to regions where there is visible content, helping to train quite faster our model."]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1651597025275,"user":{"displayName":"Guillem garrofé","userId":"02696913160405475090"},"user_tz":-60},"id":"zl-nZBtDavRz"},"outputs":[],"source":["def sample_pdf(locations, weights, N_samples=128, plt_fn=None, rays_o=None, rays_d=None):\n","    weights += 1e-5  # prevent nans\n","    pdf = weights / torch.sum(weights, dim=-1, keepdim=True)\n","    \n","    cdf = torch.cumsum(pdf, -1)\n","    cdf = torch.concat((torch.zeros_like(cdf[..., :1]), cdf), dim=-1) # prob of being between -1 and 0 is 0\n","    \n","    u = torch.rand(list(cdf.shape[:-1]) + [N_samples]).to(cdf)\n","\n","    js = torch.searchsorted(cdf, u, right=True)\n","    prev_j = torch.maximum(torch.tensor(0), js-1)\n","    j = torch.minimum(torch.tensor(cdf.shape[-1]-1), js)\n","\n","    cdf_prev_j = torch.gather(cdf, dim=-1, index=prev_j)\n","    cdf_j = torch.gather(cdf, dim=-1, index=j)\n","\n","    locations_prev_j = torch.gather(locations, dim=-1, index=prev_j)\n","    locations_j = torch.gather(locations, dim=-1, index=j)\n","\n","    denom = cdf_j-cdf_prev_j\n","    denom = torch.where(denom < 1e-5, torch.ones_like(denom), denom) # avoid division by 0\n","    t = (u-cdf_prev_j)/denom\n","    \n","    samples = locations_prev_j + t * (locations_j-locations_prev_j)\n","\n","    if plt_fn is not None and rays_o is not None and rays_d is not None:\n","        plt_fn(pdf, cdf, j, samples)\n","\n","    return samples\n","\n","if debug:\n","    z_locs_mid = .5 * (z_locations_flat[..., 1:] + z_locations_flat[..., :-1])\n","    samples = sample_pdf(z_locs_mid, \n","                        weights[..., 1:-1], \n","                        plt_fn=v.plt_first_ray_sampling_pdf, \n","                        rays_o=torch.reshape(rays_o, [-1,3]),\n","                        rays_d=torch.reshape(rays_d, [-1,3]))"]},{"cell_type":"markdown","metadata":{"id":"9Z2xNxXn_OYt"},"source":["# Train NeRF"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1q1-Cr_SaTXVWTMbyIt4F-EuTF7N3T9wV","referenced_widgets":["b82e9ed58a574d72a56cec4c6ba49467","8601a2413ada4f97bea79f67514e1169","8014a08a78724814b3c3eefcd755cb3a","c86e903f6ddf477cb4bcf5ecec747f6f","9a518817e68a4925b705cff742016dc8","ed6b457f3ed94f248570f784679ce13b","cc571460c6e9421c9c80c7298514a3b2","0a81f523a74941a3b1fa56e566fd22ac","591c597cd6a440fdbba9dbef5d5799dd","522bd3532dc4477ca71450a2d3fc09b3","e83b28a9e4884456b1aaa00a081055ce"]},"id":"GRy1Xl2gNyti","outputId":"3a2abe84-c07c-45f2-96dc-c60d18a62d3f"},"outputs":[{"data":{"text/plain":["Output hidden; open in https://colab.research.google.com to view."]},"metadata":{},"output_type":"display_data"}],"source":["def validation_view(model, pose_val, hwf, val_target, img_shape, it, N_samples=64, model_f=None, N_f=128, near=2, far=6):\n","    model.eval()\n","    pose_val = pose_val.to(hwf[2])\n","    \n","    val_rays = data.get_validation_rays(pose_val, hwf, get_rays_origins_and_directions)\n","    pred = predict(val_rays, model, N_samples=N_samples, model_f=model_f, N_f=N_f, near=near, far=far)\n","    rgb_map = pred[\"rgb_map\"] \n","    \n","    rgb_map = torch.reshape(rgb_map, img_shape)\n","    \n","    plt.figure(figsize=(10, 4))\n","    plt.subplot(121)\n","    plt.imshow(rgb_map.detach().cpu().numpy())\n","    plt.title(f\"Reconstruction - it {it}\")\n","    plt.subplot(122)\n","    plt.imshow(val_target.detach().cpu().numpy())\n","    plt.title(\"Target image\")\n","    plt.show()\n","\n","\n","def run_model(locations, view_dirs, model):\n","    locs_flat = torch.reshape(locations, [-1, locations.shape[-1]])\n","    \n","    view_dirs = torch.broadcast_to(view_dirs, locations.shape)\n","    view_dirs_flat = torch.reshape(view_dirs, [-1, view_dirs.shape[-1]])\n","\n","    raw_radiance_density_flat = model(locs_flat, view_dirs=view_dirs_flat)\n","    raw_radiance_density = torch.reshape(raw_radiance_density_flat, \n","                                            list(locations.shape[:-1]) + [raw_radiance_density_flat.shape[-1]])\n","    return raw_radiance_density\n","\n","def predict(batch_rays, model, N_samples=64, model_f=None, N_f=128, near=2, far=6, raw_noise_std=0.0):\n","    rays_o = batch_rays[..., :3]\n","    rays_d = batch_rays[..., 3:6]\n","    view_dirs = batch_rays[..., None, 6:9]\n","    locations, depths = get_sampling_locations(rays_o=rays_o, \n","                                                   rays_d=rays_d, \n","                                                   near=near, \n","                                                   far=far, \n","                                                   n_samples=N_samples)\n","    \n","    raw_radiance_density = run_model(locations, view_dirs, model)\n","    rgb_map, weights = volume_rendering(raw_radiance_density, depths, \n","                                        rays_d, raw_noise_std=raw_noise_std)\n","    \n","    if N_f > 0:\n","        rgb_map_0, weights_0 = rgb_map, weights\n","\n","        depths_mid = .5 * (depths[..., 1:] + depths[..., :-1])\n","        depths_f = sample_pdf(depths_mid, weights[..., 1:-1])\n","        depths_f = depths_f.detach()\n","        \n","        depths_new = torch.cat((depths, depths_f), dim=-1)\n","        depths_new, _ = torch.sort(depths_new, dim=-1)\n","        locs_f = rays_o[...,None,:] + rays_d[...,None,:] * depths_new[...,:,None]\n","\n","        raw_radiance_density = run_model(locs_f, view_dirs, model_f)\n","        rgb_map, weights = volume_rendering(raw_radiance_density, depths_new, \n","                                            rays_d, raw_noise_std=raw_noise_std)\n","\n","    pred = {'rgb_map': rgb_map, 'weights': weights}\n","    if N_f > 0:\n","        pred['rgb_map_0'] = rgb_map_0\n","        pred['weights_0'] = weights_0\n","    return pred\n","\n","def train(dataset=\"tiny_nerf\", batch_size=1024*1, lr=2e-5, N_iters=10000, \n","          N_samples=64, N_f=128, near=2, far=6, raw_noise_std=0.0):\n","    # Load data\n","    if dataset == \"tiny_nerf\":\n","        poses, images, hwf, i_split = data.load_tiny_nerf(\"tiny_nerf_data.npz\", device=device)\n","        i_train, i_val, i_test = i_split\n","        \n","    #Create datasets\n","    loader_train, loader_val, loader_test = data.create_dataset(poses, \n","                                                                images, \n","                                                                hwf, \n","                                                                i_split, \n","                                                                get_rays_origins_and_directions, \n","                                                                batch_size,\n","                                                                device=device)\n","\n","    # Create models\n","    model = NeRFModel()\n","    model.to(device)\n","    model_f = None\n","\n","    if N_f > 0:\n","        model_f = NeRFModel(D=2)\n","        model_f.to(device)\n","\n","    # Create optimizer\n","        optimizer = torch.optim.Adam(list(model.parameters()) + list(model_f.parameters()), lr=lr)\n","\n","    else:\n","        optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    \n","    loss_fn = nn.MSELoss()\n","\n","    # Train\n","    train_iterator = iter(loader_train)\n","    val_iterator = iter(loader_val)\n","    training_losses = []\n","    val_losses = []\n","\n","    for it in tqdm(range(N_iters), unit=\"iteration\"):\n","        try:\n","            batch_rays_tr, target_rgb = next(train_iterator)\n","        except StopIteration:\n","            train_iterator = iter(loader_train)\n","            batch_rays_tr, target_rgb = next(train_iterator)\n","\n","        try:\n","            batch_rays_val, target_rgb_val = next(val_iterator)\n","        except StopIteration:\n","            val_iterator = iter(loader_val)\n","            batch_rays_val, target_rgb_val = next(val_iterator)\n","\n","        model.train()\n","        pred = predict(batch_rays_tr, model, N_samples=N_samples, model_f=model_f, \n","                       N_f=N_f, near=near, far=far, raw_noise_std=raw_noise_std)\n","\n","        if N_f > 0:\n","            loss_c = loss_fn(pred['rgb_map_0'], target_rgb)\n","            loss_f = loss_fn(pred['rgb_map'], target_rgb)\n","            loss = loss_c + loss_f\n","        else:\n","            loss = loss_fn(pred['rgb_map'], target_rgb)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        training_losses.append(loss.item())\n","\n","        # ----- VALIDATION LOSS -----\n","        model.eval()\n","        pred = predict(batch_rays_val, model, N_samples=N_samples, model_f=model_f, N_f=N_f, near=near, far=far)\n","        \n","        if N_f > 0:\n","            loss_c = loss_fn(pred['rgb_map_0'], target_rgb_val)\n","            loss_f = loss_fn(pred['rgb_map'], target_rgb_val)\n","            loss_val = loss_c + loss_f\n","        else:\n","            loss_val = loss_fn(pred['rgb_map'], target_rgb_val)\n","\n","        val_losses.append(loss_val.item())\n","        \n","        if it%500 == 0:\n","            validation_view(model, poses[i_val[0]], hwf, images[i_val[0]], it=it, img_shape=images.shape[-3:], \n","                            N_samples=N_samples, model_f=model_f, N_f=N_f, near=near, far=far)\n","            v.plot_losses(training_losses, val_losses, it=it)\n","\n","torch.autograd.set_detect_anomaly(True)                      \n","DATASET = \"tiny_nerf\" # tiny_nerf\n","train(dataset=DATASET, N_iters=100000, batch_size=1024, lr=5e-4, N_f=0, raw_noise_std=.0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ScPU_6aafw-g"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"machine_shape":"hm","name":"Learning_NeRF.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.10"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0a81f523a74941a3b1fa56e566fd22ac":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"522bd3532dc4477ca71450a2d3fc09b3":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"591c597cd6a440fdbba9dbef5d5799dd":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"8014a08a78724814b3c3eefcd755cb3a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_0a81f523a74941a3b1fa56e566fd22ac","max":100000,"min":0,"orientation":"horizontal","style":"IPY_MODEL_591c597cd6a440fdbba9dbef5d5799dd","value":6754}},"8601a2413ada4f97bea79f67514e1169":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ed6b457f3ed94f248570f784679ce13b","placeholder":"​","style":"IPY_MODEL_cc571460c6e9421c9c80c7298514a3b2","value":"  7%"}},"9a518817e68a4925b705cff742016dc8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b82e9ed58a574d72a56cec4c6ba49467":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8601a2413ada4f97bea79f67514e1169","IPY_MODEL_8014a08a78724814b3c3eefcd755cb3a","IPY_MODEL_c86e903f6ddf477cb4bcf5ecec747f6f"],"layout":"IPY_MODEL_9a518817e68a4925b705cff742016dc8"}},"c86e903f6ddf477cb4bcf5ecec747f6f":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_522bd3532dc4477ca71450a2d3fc09b3","placeholder":"​","style":"IPY_MODEL_e83b28a9e4884456b1aaa00a081055ce","value":" 6754/100000 [12:49&lt;2:52:07,  9.03iteration/s]"}},"cc571460c6e9421c9c80c7298514a3b2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e83b28a9e4884456b1aaa00a081055ce":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ed6b457f3ed94f248570f784679ce13b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
